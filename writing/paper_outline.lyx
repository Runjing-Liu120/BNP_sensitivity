#LyX 2.3 created this file. For more info see http://www.lyx.org/
\lyxformat 544
\begin_document
\begin_header
\save_transient_properties true
\origin unavailable
\textclass article
\use_default_options true
\begin_modules
theorems-ams-chap-bytype
theorems-ams-extended-chap-bytype
\end_modules
\maintain_unincluded_children false
\language english
\language_package default
\inputencoding auto
\fontencoding global
\font_roman "default" "default"
\font_sans "default" "default"
\font_typewriter "default" "default"
\font_math "auto" "auto"
\font_default_family default
\use_non_tex_fonts false
\font_sc false
\font_osf false
\font_sf_scale 100 100
\font_tt_scale 100 100
\use_microtype false
\use_dash_ligatures true
\graphics default
\default_output_format default
\output_sync 0
\bibtex_command default
\index_command default
\paperfontsize default
\spacing single
\use_hyperref false
\papersize default
\use_geometry false
\use_package amsmath 1
\use_package amssymb 1
\use_package cancel 1
\use_package esint 1
\use_package mathdots 1
\use_package mathtools 1
\use_package mhchem 1
\use_package stackrel 1
\use_package stmaryrd 1
\use_package undertilde 1
\cite_engine basic
\cite_engine_type default
\biblio_style plain
\use_bibtopic false
\use_indices false
\paperorientation portrait
\suppress_date false
\justification true
\use_refstyle 1
\use_minted 0
\index Index
\shortcut idx
\color #008000
\end_index
\secnumdepth 3
\tocdepth 3
\paragraph_separation indent
\paragraph_indentation default
\is_math_indent 0
\math_numbering_side default
\quotes_style english
\dynamic_quotes 0
\papercolumns 1
\papersides 1
\paperpagestyle default
\tracking_changes false
\output_changes false
\html_math_output 0
\html_css_as_file 0
\html_be_strict false
\end_header

\begin_body

\begin_layout Standard
\begin_inset FormulaMacro
\newcommand{\basepost}[1][\theta]{p\left(#1\vert X\right)}
{p\left(#1\vert X\right)}
\end_inset


\end_layout

\begin_layout Standard
\begin_inset FormulaMacro
\newcommand{\baseprior}[1][\theta]{p_{0}\left(#1\right)}
{p_{0}\left(#1\right)}
\end_inset


\end_layout

\begin_layout Standard
\begin_inset FormulaMacro
\newcommand{\contamprior}[2][\theta][\epsilon,\phi]{p_{c}\left(#1\vert#2\right)}
{p_{c}\left(#1\vert#2\right)}
\end_inset


\end_layout

\begin_layout Standard
\begin_inset FormulaMacro
\newcommand{\priorpert}[1][\theta]{\phi\left(#1\right)}
{\phi\left(#1\right)}
\end_inset


\end_layout

\begin_layout Standard
\begin_inset FormulaMacro
\newcommand{\post}[2][\theta][\epsilon,\phi]{p\left(#1\vert X,#2\right)}
{p\left(#1\vert X,#2\right)}
\end_inset


\end_layout

\begin_layout Standard
\begin_inset FormulaMacro
\newcommand{\qpost}[2][\theta][\eta^{*}]{q\left(#1;#2\right)}
{q\left(#1;#2\right)}
\end_inset


\end_layout

\begin_layout Standard
\begin_inset FormulaMacro
\newcommand{\logpert}[2][\theta][\alpha]{\rho\left(#1,#2\right)}
{\rho\left(#1;#2\right)}
\end_inset


\end_layout

\begin_layout Standard
\begin_inset FormulaMacro
\newcommand{\targetfun}{g\left(\theta\right)}
{g\left(\theta\right)}
\end_inset


\end_layout

\begin_layout Standard
\begin_inset FormulaMacro
\newcommand{\etaopt}{\eta^{*}}
{\eta^{*}}
\end_inset


\end_layout

\begin_layout Standard
\begin_inset FormulaMacro
\newcommand{\klhess}{H}
{H}
\end_inset


\end_layout

\begin_layout Standard
\begin_inset FormulaMacro
\newcommand{\eggrad}{G}
{G}
\end_inset


\end_layout

\begin_layout Standard
\begin_inset FormulaMacro
\newcommand{\mbe}{\mathbb{E}}
{\mathbb{E}}
\end_inset


\end_layout

\begin_layout Standard
\begin_inset FormulaMacro
\newcommand{\cov}{\mathrm{Cov}}
{\mathrm{Cov}}
\end_inset


\end_layout

\begin_layout Standard
\begin_inset FormulaMacro
\newcommand{\pinfluenceop}[1][\phi]{\mathbb{I}_{p}\left[#1\right]}
{\mathbb{I}_{p}\left[#1\right]}
\end_inset


\end_layout

\begin_layout Standard
\begin_inset FormulaMacro
\newcommand{\qinfluenceop}[1][\phi]{\mathbb{I}_{q}\left[#1\right]}
{\mathbb{I}_{q}\left[#1\right]}
\end_inset


\end_layout

\begin_layout Standard
\begin_inset FormulaMacro
\newcommand{\pinfluencefun}[1][\theta]{\mathcal{I}_{p}\left(#1\right)}
{\mathcal{I}_{p}\left(#1\right)}
\end_inset


\end_layout

\begin_layout Standard
\begin_inset FormulaMacro
\newcommand{\qinfluencefun}[1][\theta]{\mathcal{I}_{q}\left(#1\right)}
{\mathcal{I}_{q}\left(#1\right)}
\end_inset


\end_layout

\begin_layout Standard
\begin_inset FormulaMacro
\newcommand{\thetadom}{\Omega_{\theta}}
{\Omega_{\theta}}
\end_inset


\end_layout

\begin_layout Standard
\begin_inset FormulaMacro
\newcommand{\etadom}{\Omega_{\eta}}
{\Omega_{\eta}}
\end_inset


\end_layout

\begin_layout Standard
\begin_inset FormulaMacro
\newcommand{\qdom}{\mathcal{Q}}
{\mathcal{Q}}
\end_inset


\end_layout

\begin_layout Standard
\begin_inset FormulaMacro
\newcommand{\pertbound}{C_{\phi}}
{C_{\phi}}
\end_inset


\end_layout

\begin_layout Section
Introduction
\end_layout

\begin_layout Standard
A central question in many probabilistic clustering problems is how many
 distinct clusters are present in a particular dataset.
 In all but the simplest problems, any attempt to answer this question may
 be strongly determined by the criterion used.
 One common approach, based in the Bayesian tradition, addresses the problem
 with a generative model: out of a population of unobserved latent clusters,
 some finite number are randomly chosen to be present in the actual data
 at hand.
 The identity and number of these clusters can then be estimated – with
 attendant uncertainty estimates – using the tools of Bayesian posterior
 inference.
 For example, one might estimate the 
\begin_inset Quotes eld
\end_inset

number of distinct clusters present
\begin_inset Quotes erd
\end_inset

 by its posterior expectation.
 Such a generative model is called a 
\begin_inset Quotes eld
\end_inset

Bayesian non-parametric
\begin_inset Quotes erd
\end_inset

 (BNP) model when the number of latent clusters is infinite, though, naturally,
 even in non-parametric models only a finite number of clusters can actually
 be observed in any particular dataset.
 
\end_layout

\begin_layout Standard
As with any Bayesian model, this approach requires the specification of
 a prior and a likelihood.
 In this case, the likelihood describes the dispersion of data within a
 particular cluster, and the prior determines both the distribution of cluster
 shapes and sizes as well as the process that determines how many clusters
 are present.
 In general, different choices of the prior and likelihood would give different
 answers to the question 
\begin_inset Quotes eld
\end_inset

how many distinct clusters are present?
\begin_inset Quotes erd
\end_inset

 For example, if the prior does not somehow prefer fewer, larger clusters,
 then there is nothing that inherently prevents such an approach from inferring
 that each datapoint is in its own cluster.
 However, one still hopes that a broad range of reasonable choices of prior
 and likelihood will come to similar conclusions.
 Consequently, it is important, in practice, to measure the sensitivity
 of the inferred number of clusters present to the prior and likelihood
 specification.
 Furthermore, these sensitivity measures should work with the kinds of inference
 tools that are used in practice, operate relatively automatically without
 re-fitting the model many times, and measure sensitivity not only to alternativ
e hyperparameters but also to alternative functional forms of the prior
 and likelihood.
 
\end_layout

\begin_layout Standard
To address these needs, we develop fast, automatic measures of the sensitivity
 of variational Bayes (VB) approximations to perturbations of functional
 forms in a putative model.
 As a motivating application, we apply our techniques to estimate the sensitivit
y of BNP posteriors to the functional form of a particular BNP prior known
 as the stick-breaking prior.
 As we will see below, stick-breaking priors provide a strong motivation
 to quantify functional perturbations.
 A typical choice of a stick breaking prior is specified with only a single
 real-valued hyperparameter and also a potentially informative distributional
 assumption, the form of a stick breaking prior can substantially inform
 the number of clusters inferred to be present in a particular dataset,
 and it is arguably difficult for ordinary practitioners to form meaningful
 subjective beliefs about the abstract form of the stick breaking prior.
\end_layout

\begin_layout Standard
We begin by deriving a general result for the sensitivity of VB optima to
 function-valued perturbations, as well as several useful specializations.
 We then describe a VB approximation to a BNP model with a stick-breaking
 prior and derive the sensitivity of the approximate number of inferred
 clusters to the choice of the stick breaking prior.
 We then apply our methods to a real dataset, comparing our results to the
 much more expensive process of re-fitting the model.
\end_layout

\begin_layout Section
Functional sensitivity
\end_layout

\begin_layout Subsection
Variational Bayes
\end_layout

\begin_layout Standard
Suppose we observe data, 
\begin_inset Formula $X$
\end_inset

, and a parameter 
\begin_inset Formula $\theta\in\thetadom\subseteq\mathbb{R}^{K}$
\end_inset

, which parameterizes a generative model for 
\begin_inset Formula $X$
\end_inset

, denoted 
\begin_inset Formula $p\left(X\vert\theta\right)$
\end_inset

.
 Here, 
\begin_inset Formula $K$
\end_inset

 may be finite or, as in the case of BNP, countably infinite.
 Defining a base prior, 
\begin_inset Formula $\baseprior$
\end_inset

, with respect to a dominating measure 
\begin_inset Formula $\lambda$
\end_inset

 on 
\begin_inset Formula $\thetadom$
\end_inset

, we can calculate the posterior distribution on 
\begin_inset Formula $\theta$
\end_inset

 given 
\begin_inset Formula $X$
\end_inset

:
\begin_inset Formula 
\begin{align}
\basepost & :=\frac{p\left(X\vert\theta\right)\baseprior}{\int p\left(X\vert\theta'\right)\baseprior[\theta']\lambda\left(d\theta'\right)}=\frac{p\left(X\vert\theta\right)\baseprior}{p\left(X\right)}.\label{eq:posterior_defn}
\end{align}

\end_inset

Suppose we are interested in calculating a posterior expectation of some
 function of interest, say 
\begin_inset Formula $\mbe_{\basepost}\left[\targetfun\right]$
\end_inset

.
 In many cases (such as most clustering problems), 
\begin_inset Formula $\basepost$
\end_inset

 is intractable and must be approximated.
 One popular approximation technique, known as variational Bayes (VB), approxima
tes 
\begin_inset Formula $\basepost$
\end_inset

 in KL divergence with a distribution in some convenient sub-class of all
 possible distributions.
 Let 
\begin_inset Formula $\qdom$
\end_inset

 be a set of distributions parameterized by the real-valued parameter 
\begin_inset Formula $\eta\in\etadom\subseteq\mathbb{R}^{K_{q}}$
\end_inset

:
\begin_inset Formula 
\begin{align*}
\qdom & :=\left\{ q:q=\qpost[][\eta]\textrm{ for }\eta\in\etadom\right\} .
\end{align*}

\end_inset

For example, 
\begin_inset Formula $\qdom$
\end_inset

 might be a convenient exponential family such as a factorizing normal distribut
ion, in which case 
\begin_inset Formula $\eta$
\end_inset

 would parameterize the mean and covariance of 
\begin_inset Formula $\qpost[][\eta]$
\end_inset

.
 The optimal parameter, 
\begin_inset Formula $\etaopt$
\end_inset

, is the parameter that minimizes the KL divergence between 
\begin_inset Formula $\qpost[][\eta]$
\end_inset

 and 
\begin_inset Formula $\basepost$
\end_inset

:
\begin_inset Formula 
\begin{align}
KL\left(\qpost[][\eta]||\basepost\right) & :=\mbe_{\qpost[][\eta]}\left[\log\qpost[][\eta]-\log\basepost\right]\nonumber \\
\etaopt & :=\underset{\eta\in\etadom}{\mathrm{argmin}}KL\left(\qpost[][\eta]||\basepost\right).\label{eq:kl_opt}
\end{align}

\end_inset

The distribution 
\begin_inset Formula $\qpost$
\end_inset

 is then the closest distribution in 
\begin_inset Formula $\qdom$
\end_inset

 to 
\begin_inset Formula $\basepost$
\end_inset

 as measured by KL divergence, and is called the 
\begin_inset Quotes eld
\end_inset

variational approximation
\begin_inset Quotes erd
\end_inset

 to 
\begin_inset Formula $\basepost$
\end_inset

.
 Noting that
\begin_inset Formula 
\begin{align*}
\mbe_{\qpost[][\eta]}\left[\log\basepost\right] & =\mbe_{\qpost[][\eta]}\left[\log p\left(X\vert\theta\right)+\log\baseprior\right]-\log p\left(X\right),
\end{align*}

\end_inset

we see that the intractable normalizing constant of 
\begin_inset Formula $\basepost$
\end_inset

 can be neglected when optimizing to find 
\begin_inset Formula $\etaopt$
\end_inset

.
 We then hope that 
\begin_inset Formula $\qdom$
\end_inset

 is sufficiently expressive that
\begin_inset Formula 
\begin{align*}
\mbe_{\qpost}\left[\targetfun\right] & \approx\mbe_{\basepost}\left[\targetfun\right].
\end{align*}

\end_inset


\end_layout

\begin_layout Subsection
Prior sensitivity
\end_layout

\begin_layout Standard
The posterior expectation 
\begin_inset Formula $\mbe_{\basepost}\left[\targetfun\right]$
\end_inset

 depends on the choice of prior, 
\begin_inset Formula $\baseprior$
\end_inset

, through the definition of the posterior in 
\begin_inset CommandInset ref
LatexCommand formatted
reference "eq:posterior_defn"
plural "false"
caps "false"
noprefix "false"

\end_inset

.
 Similarly, the VB approximate posterior expectation 
\begin_inset Formula $\mbe_{\qpost}\left[\targetfun\right]$
\end_inset

 depends on the choice of 
\begin_inset Formula $\baseprior$
\end_inset

 through its occurrence in the objective of the optimization problem 
\begin_inset CommandInset ref
LatexCommand formatted
reference "eq:kl_opt"
plural "false"
caps "false"
noprefix "false"

\end_inset

.
 In order to quantify this dependence, we will follow GUSTAFSON, defining
 a class of perturbations to 
\begin_inset Formula $\baseprior$
\end_inset

 and quantifying the sensitivity of 
\begin_inset Formula $\mbe_{\qpost}\left[\targetfun\right]$
\end_inset

 to these perturbations.
\end_layout

\begin_layout Standard
Consider a bounded 
\begin_inset Formula $\lambda$
\end_inset

-integrable perturbation 
\begin_inset Formula $\priorpert$
\end_inset

 (which may be positive or negative), take some 
\begin_inset Formula $\epsilon\ge0$
\end_inset

, and define the 
\begin_inset Quotes eld
\end_inset

contamination prior
\begin_inset Quotes erd
\end_inset

.
\end_layout

\begin_layout Definition
Let 
\begin_inset Formula $\priorpert$
\end_inset

 be a bounded, 
\begin_inset Formula $\lambda$
\end_inset

-measurable and 
\begin_inset Formula $\lambda$
\end_inset

-integrable function, i.e., a function for which there exists a 
\begin_inset Formula $\pertbound<\infty$
\end_inset

 with
\begin_inset Formula 
\begin{align*}
\sup_{\theta\in\thetadom}\priorpert & \le\pertbound\\
\int\left|\priorpert\right|\lambda\left(d\theta\right) & \le\pertbound.
\end{align*}

\end_inset


\begin_inset Formula 
\begin{align*}
\contamprior & :=\frac{\baseprior+\epsilon\priorpert}{\int\left(\baseprior[\theta']+\epsilon\priorpert[\theta']\right)\lambda\left(d\theta'\right)}.
\end{align*}

\end_inset


\end_layout

\begin_layout Standard
\begin_inset Separator plain
\end_inset


\end_layout

\begin_layout Standard
As long as 
\begin_inset Formula $\epsilon$
\end_inset

 is chosen so that 
\begin_inset Formula $\baseprior+\epsilon\priorpert\ge0$
\end_inset

, the distribution 
\begin_inset Formula $\contamprior$
\end_inset

 is a well-defined distribution.
 Because 
\begin_inset Formula $\priorpert$
\end_inset

 is bounded, there always exists an open interval for 
\begin_inset Formula $\epsilon$
\end_inset

 such that 
\begin_inset Formula $\contamprior$
\end_inset

 is well-defined.
 By using 
\begin_inset Formula $\contamprior$
\end_inset

 as our prior, for any 
\begin_inset Formula $\epsilon$
\end_inset

 and 
\begin_inset Formula $\phi$
\end_inset

 we can define the 
\begin_inset Quotes eld
\end_inset

contamination posterior
\begin_inset Quotes erd
\end_inset

:
\begin_inset Formula 
\begin{align*}
\post & :=\frac{p\left(X\vert\theta\right)\contamprior}{\int p\left(X\vert\theta'\right)\contamprior[\theta']\lambda\left(d\theta\right)}.
\end{align*}

\end_inset

For a given 
\begin_inset Formula $\phi$
\end_inset

, the posterior expectation 
\begin_inset Formula $\mbe_{\post}\left[\targetfun\right]$
\end_inset

 depends on 
\begin_inset Formula $\epsilon$
\end_inset

, and this dependence measures the effect of perturbing the original prior
 
\begin_inset Formula $\baseprior$
\end_inset

 by 
\begin_inset Formula $\priorpert$
\end_inset

.
 
\end_layout

\begin_layout Standard
For example, suppose we were interested in the effect of replacing the prior
 
\begin_inset Formula $\baseprior$
\end_inset

 with an alternative distribution, 
\begin_inset Formula $p_{1}\left(\theta\right)$
\end_inset

.
 If we take
\begin_inset Formula 
\begin{align*}
\priorpert & =p_{1}\left(\theta\right)-\baseprior\Rightarrow\\
\contamprior & =\left(1-\epsilon\right)\baseprior+\epsilon p_{1}\left(\theta\right),
\end{align*}

\end_inset

we can see that 
\begin_inset Formula $\contamprior$
\end_inset

 is an 
\begin_inset Formula $\epsilon$
\end_inset

-additive mixture of 
\begin_inset Formula $\baseprior$
\end_inset

 and 
\begin_inset Formula $p_{1}\left(\theta\right)$
\end_inset

.
 Denote with 
\begin_inset Formula $p_{1}\left(\theta\vert X\right)$
\end_inset

 the posterior as calculated using the prior 
\begin_inset Formula $p_{1}\left(\theta\right)$
\end_inset

.
 Then the difference in the expectation of 
\begin_inset Formula $\targetfun$
\end_inset

 between the two priors is given by
\begin_inset Formula 
\begin{align*}
\mbe_{p_{1}\left(\theta\vert X\right)}\left[\targetfun\right]-\mbe_{\basepost}\left[\targetfun\right] & =\mbe_{\post[][\epsilon=1,\phi]}\left[\targetfun\right]-\mbe_{\post[][\epsilon=0,\phi]}\left[\targetfun\right].
\end{align*}

\end_inset

Calculating 
\begin_inset Formula $\mbe_{p_{1}\left(\theta\vert X\right)}\left[\targetfun\right]$
\end_inset

 exactly may be expensive – for example, if 
\begin_inset Formula $p_{1}\left(\theta\vert X\right)$
\end_inset

 is far from 
\begin_inset Formula $\basepost$
\end_inset

, then importance sampling may have very few effective samples.
 However, we can approximate the difference using a first-order approximation:
\begin_inset Formula 
\begin{align}
\mbe_{p_{1}\left(\theta\vert X\right)}\left[\targetfun\right]-\mbe_{\basepost}\left[\targetfun\right] & \approx\left.\frac{d}{d\epsilon}\mbe_{\post}\left[\targetfun\right]\right|_{\epsilon=0}\left(1-0\right).\label{eq:difference_approx}
\end{align}

\end_inset

The derivative in 
\begin_inset CommandInset ref
LatexCommand formatted
reference "eq:difference_approx"
plural "false"
caps "false"
noprefix "false"

\end_inset

 is known as 
\begin_inset Quotes eld
\end_inset

local sensitivity
\begin_inset Quotes erd
\end_inset

 to 
\begin_inset Formula $\epsilon$
\end_inset

, and the derivative depends on 
\begin_inset Formula $\phi$
\end_inset

.
 
\end_layout

\begin_layout Standard
As discussed in GIORDANO, for a log-perturbed posterior
\begin_inset Formula 
\begin{align*}
\post[][\epsilon] & :=\frac{\basepost\exp\left(\rho\left(\theta,\epsilon\right)\right)}{\int\basepost[\theta']\exp\left(\rho\left(\theta',\epsilon\right)\right)\lambda\left(d\theta'\right)},
\end{align*}

\end_inset

with variational approximation 
\begin_inset Formula $\qpost[][\etaopt\left(\epsilon,\phi\right)]$
\end_inset

 given by
\begin_inset Formula 
\begin{align*}
\etaopt\left(\epsilon,\phi\right) & :=\underset{\eta\in\etadom}{\mathrm{argmin}}KL\left(\qpost[][\eta]||\post\right),
\end{align*}

\end_inset

then
\begin_inset Formula 
\begin{align*}
\left.\frac{d}{d\epsilon}\mbe_{\post[][\epsilon]}\left[\targetfun\right]\right|_{\epsilon=0} & =\cov_{\basepost}\left(\targetfun,\left.\frac{\partial}{\partial\epsilon}\rho\left(\theta,\epsilon\right)\right|_{\epsilon=0}\right)
\end{align*}

\end_inset

and
\begin_inset Formula 
\begin{align*}
\klhess & :=\left.\frac{\partial KL\left(\eta\right)}{\partial\eta\partial\eta^{T}}\right|_{\eta=\etaopt\left(0,\phi\right)}\\
\eggrad & :=\left.\frac{\partial\mbe_{\qpost[][\eta]}\left[\targetfun\right]}{\partial\eta^{T}}\right|_{\eta=\etaopt\left(0,\phi\right)}\\
\left.\frac{d}{d\epsilon}\mbe_{\qpost[][\etaopt\left(\epsilon,\phi\right)]}\left[\targetfun\right]\right|_{\epsilon=0} & =-\eggrad\klhess^{-1}\left.\frac{\partial}{\partial\eta}\mbe_{\qpost[][\eta]}\left[\left.\frac{\partial}{\partial\epsilon}\rho\left(\theta,\epsilon\right)\right|_{\epsilon=0}\right]\right|_{\eta=\etaopt\left(0,\phi\right)}.
\end{align*}

\end_inset

Using 
\begin_inset Formula $\contamprior$
\end_inset

 in place of 
\begin_inset Formula $\baseprior$
\end_inset

 is equivalent to taking 
\begin_inset Formula 
\begin{align*}
\rho\left(\theta,\epsilon\right) & =\log\post[][\epsilon]-\log\baseprior\Rightarrow\\
\frac{\partial}{\partial\epsilon}\rho\left(\theta,\epsilon\right) & =\frac{\partial}{\partial\epsilon}\log\left(\baseprior+\epsilon\priorpert\right)-\frac{\partial}{\partial\epsilon}\log\int\left(\baseprior[\theta']+\epsilon\priorpert[\theta']\right)\lambda\left(d\theta'\right)\\
 & =\frac{\priorpert}{\baseprior+\epsilon\priorpert}-\frac{\int\priorpert[\theta']\lambda\left(d\theta'\right)}{\int\left(\baseprior[\theta']+\epsilon\priorpert[\theta']\right)\lambda\left(d\theta'\right)}\Rightarrow\\
\left.\frac{\partial}{\partial\epsilon}\rho\left(\theta,\epsilon\right)\right|_{\epsilon=0} & =\frac{\priorpert}{\baseprior}-\int\priorpert[\theta']\lambda\left(d\theta'\right).
\end{align*}

\end_inset

Noting that the term 
\begin_inset Formula $\int\priorpert[\theta']\lambda\left(d\theta'\right)$
\end_inset

 is not a function of 
\begin_inset Formula $\theta$
\end_inset

 , we have that
\end_layout

\begin_layout Proposition
\begin_inset CommandInset label
LatexCommand label
name "prop:sensitivity_formulas"

\end_inset


\begin_inset Formula 
\begin{align}
\left.\frac{d}{d\epsilon}\mbe_{\post}\left[\targetfun\right]\right|_{\epsilon=0} & =\cov_{\basepost}\left(\targetfun,\frac{\priorpert}{\baseprior}\right)\label{eq:posterior_sensitivity_formula}\\
\left.\frac{d}{d\epsilon}\mbe_{\qpost[][\etaopt\left(\epsilon,\phi\right)]}\left[\targetfun\right]\right|_{\epsilon=0} & =-\eggrad\klhess^{-1}\left.\frac{\partial}{\partial\eta}\mbe_{\qpost[][\eta]}\left[\frac{\priorpert}{\baseprior}\right]\right|_{\eta=\etaopt\left(0,\phi\right)}.\label{eq:vb_sensitivity_formula}
\end{align}

\end_inset


\end_layout

\begin_layout Standard
\begin_inset Separator plain
\end_inset


\end_layout

\begin_layout Standard
\begin_inset CommandInset ref
LatexCommand formatted
reference "eq:posterior_sensitivity_formula"
plural "false"
caps "false"
noprefix "false"

\end_inset

 is given in GUSTAFSON; our contribution is the corresponding 
\begin_inset CommandInset ref
LatexCommand formatted
reference "eq:vb_sensitivity_formula"
plural "false"
caps "false"
noprefix "false"

\end_inset

 for variational approximations.
 
\end_layout

\begin_layout Subsection
Worst-case perturbations
\end_layout

\begin_layout Standard
The sensitivities in 
\begin_inset CommandInset ref
LatexCommand formatted
reference "prop:sensitivity_formulas"
plural "false"
caps "false"
noprefix "false"

\end_inset

 are defined for a particular 
\begin_inset Formula $\priorpert$
\end_inset

.
 There are many functions 
\begin_inset Formula $\priorpert$
\end_inset

 to try, and it can be useful to consider the worst-case perturbation in
 some sense.
 In this section, we develop these ideas, again following GUSTAFSON.
\end_layout

\begin_layout Standard
First, we observe that the sensitivities in 
\begin_inset CommandInset ref
LatexCommand formatted
reference "prop:sensitivity_formulas"
plural "false"
caps "false"
noprefix "false"

\end_inset

 are a functional of 
\begin_inset Formula $\priorpert$
\end_inset

: 
\end_layout

\begin_layout Standard
\begin_inset Formula 
\begin{align*}
\pinfluenceop & :=\left.\frac{d}{d\epsilon}\mbe_{\post}\left[\targetfun\right]\right|_{\epsilon=0}\\
\qinfluenceop & :=\left.\frac{d}{d\epsilon}\mbe_{\qpost[][\etaopt\left(\epsilon,\phi\right)]}\left[\targetfun\right]\right|_{\epsilon=0}.
\end{align*}

\end_inset


\end_layout

\begin_layout Standard
By plugging in the expressions from 
\begin_inset CommandInset ref
LatexCommand formatted
reference "prop:sensitivity_formulas"
plural "false"
caps "false"
noprefix "false"

\end_inset

, we can define
\begin_inset Formula 
\begin{align*}
\pinfluencefun & :=\left(\targetfun-\mbe_{\basepost}\left[\targetfun\right]\right)\frac{\basepost}{\baseprior}\\
\qinfluencefun & :=-\eggrad\klhess^{-1}\left.\frac{\partial}{\partial\eta}\log\qpost[][\eta]\right|_{\eta=\etaopt\left(0,\phi\right)}\frac{\qpost}{\baseprior}
\end{align*}

\end_inset


\end_layout

\begin_layout Standard
and define the inner product
\begin_inset Formula 
\begin{align*}
\left\langle a\left(\theta\right),b\left(\theta\right)\right\rangle  & :=\int a\left(\theta\right)b\left(\theta\right)\lambda\left(d\theta\right),
\end{align*}

\end_inset

then
\end_layout

\begin_layout Proposition
The Gateaux derivatives are given by
\end_layout

\begin_layout Proposition
\begin_inset Formula 
\begin{align*}
\pinfluenceop & =\left\langle \pinfluencefun,\priorpert\right\rangle \\
\qinfluenceop & =\left\langle \qinfluencefun,\priorpert\right\rangle .
\end{align*}

\end_inset


\end_layout

\begin_layout Standard
\begin_inset Separator plain
\end_inset


\end_layout

\begin_layout Standard

\end_layout

\begin_layout Standard
Viewing the quantities 
\begin_inset Formula $\mbe_{\basepost}\left[\targetfun\right]$
\end_inset

 and 
\begin_inset Formula $\mbe_{\qpost}\left[\targetfun\right]$
\end_inset

 as functionals of the unnormalized prior, the derivatives of 
\begin_inset CommandInset ref
LatexCommand formatted
reference "prop:sensitivity_formulas"
plural "false"
caps "false"
noprefix "false"

\end_inset

 are the Gateaux derivatives of the respective expectations in the direction
 
\begin_inset Formula $\priorpert$
\end_inset

 evaluated at 
\begin_inset Formula $\baseprior$
\end_inset

.
 Furthermore, 
\begin_inset CommandInset ref
LatexCommand formatted
reference "prop:sensitivity_formulas"
plural "false"
caps "false"
noprefix "false"

\end_inset

 gives these Gateaux derivatives as given by closed-form linear operators
 in an appropriately defined Hilbert space.
 Specifically, defining
\end_layout

\begin_layout Standard
Then we have
\begin_inset Formula 
\begin{align*}
\cov_{\basepost}\left(\targetfun,\frac{\priorpert}{\baseprior}\right) & =\int\left(\targetfun-\mbe_{\basepost}\left[\targetfun\right]\right)\frac{\priorpert}{\baseprior}\lambda\left(d\theta\right)\\
I_{p}
\end{align*}

\end_inset


\end_layout

\begin_layout Section
Extra notes
\end_layout

\begin_layout Standard
\begin_inset Formula 
\begin{align*}
-\eggrad\klhess^{-1}\left.\frac{\partial}{\partial\eta}\mbe_{\qpost[][\eta]}\left[\frac{\priorpert}{\baseprior}\right]\right|_{\eta=\etaopt\left(0,\phi\right)} & =-\eggrad\klhess^{-1}\left.\frac{\partial}{\partial\eta}\int\qpost\frac{\priorpert}{\baseprior}\lambda\left(d\theta\right)\right|_{\eta=\etaopt\left(0,\phi\right)}\\
 & =-\eggrad\klhess^{-1}\int\qpost\left.\frac{\partial}{\partial\eta}\log\qpost[][\eta]\right|_{\eta=\etaopt\left(0,\phi\right)}\frac{\priorpert}{\baseprior}\lambda\left(d\theta\right)
\end{align*}

\end_inset


\end_layout

\end_body
\end_document
