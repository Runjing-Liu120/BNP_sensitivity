#LyX 2.3 created this file. For more info see http://www.lyx.org/
\lyxformat 544
\begin_document
\begin_header
\save_transient_properties true
\origin unavailable
\textclass article
\use_default_options true
\maintain_unincluded_children false
\language english
\language_package default
\inputencoding auto
\fontencoding global
\font_roman "default" "default"
\font_sans "default" "default"
\font_typewriter "default" "default"
\font_math "auto" "auto"
\font_default_family default
\use_non_tex_fonts false
\font_sc false
\font_osf false
\font_sf_scale 100 100
\font_tt_scale 100 100
\use_microtype false
\use_dash_ligatures true
\graphics default
\default_output_format default
\output_sync 0
\bibtex_command default
\index_command default
\paperfontsize default
\use_hyperref false
\papersize default
\use_geometry false
\use_package amsmath 1
\use_package amssymb 1
\use_package cancel 1
\use_package esint 1
\use_package mathdots 1
\use_package mathtools 1
\use_package mhchem 1
\use_package stackrel 1
\use_package stmaryrd 1
\use_package undertilde 1
\cite_engine basic
\cite_engine_type default
\biblio_style plain
\use_bibtopic false
\use_indices false
\paperorientation portrait
\suppress_date false
\justification true
\use_refstyle 1
\use_minted 0
\index Index
\shortcut idx
\color #008000
\end_index
\secnumdepth 3
\tocdepth 3
\paragraph_separation indent
\paragraph_indentation default
\is_math_indent 0
\math_numbering_side default
\quotes_style english
\dynamic_quotes 0
\papercolumns 1
\papersides 1
\paperpagestyle default
\tracking_changes false
\output_changes false
\html_math_output 0
\html_css_as_file 0
\html_be_strict false
\end_header

\begin_body

\begin_layout Standard
\begin_inset FormulaMacro
\newcommand{\basepost}[1][\theta]{p\left(#1\vert X\right)}
{p\left(#1\vert X\right)}
\end_inset


\end_layout

\begin_layout Standard
\begin_inset FormulaMacro
\newcommand{\baseprior}[1][\theta]{p_{0}\left(#1\right)}
{p_{0}\left(#1\right)}
\end_inset


\end_layout

\begin_layout Standard
\begin_inset FormulaMacro
\newcommand{\contamprior}[2][\theta][\epsilon]{p_{c}\left(#1\vert#2\right)}
{p_{c}\left(#1\vert#2\right)}
\end_inset


\end_layout

\begin_layout Standard
\begin_inset FormulaMacro
\newcommand{\priorpert}[1][\theta]{\phi\left(#1\right)}
{\phi\left(#1\right)}
\end_inset


\end_layout

\begin_layout Standard
\begin_inset FormulaMacro
\newcommand{\post}[2][\theta][\epsilon]{p\left(#1\vert X,#2\right)}
{p\left(#1\vert X,#2\right)}
\end_inset


\end_layout

\begin_layout Standard
\begin_inset FormulaMacro
\newcommand{\qpost}[2][\theta][\eta^{*}]{q\left(#1;#2\right)}
{q\left(#1;#2\right)}
\end_inset


\end_layout

\begin_layout Standard
\begin_inset FormulaMacro
\newcommand{\logpert}[2][\theta][\alpha]{\rho\left(#1,#2\right)}
{\rho\left(#1;#2\right)}
\end_inset


\end_layout

\begin_layout Standard
\begin_inset FormulaMacro
\newcommand{\targetfun}{g\left(\theta\right)}
{g\left(\theta\right)}
\end_inset


\end_layout

\begin_layout Standard
\begin_inset FormulaMacro
\newcommand{\etaopt}{\eta^{*}}
{\eta^{*}}
\end_inset


\end_layout

\begin_layout Standard
\begin_inset FormulaMacro
\newcommand{\mbe}{\mathbb{E}}
{\mathbb{E}}
\end_inset


\end_layout

\begin_layout Standard
\begin_inset FormulaMacro
\newcommand{\thetadom}{\Omega_{\theta}}
{\Omega_{\theta}}
\end_inset


\end_layout

\begin_layout Standard
\begin_inset FormulaMacro
\newcommand{\etadom}{\Omega_{\eta}}
{\Omega_{\eta}}
\end_inset


\end_layout

\begin_layout Standard
\begin_inset FormulaMacro
\newcommand{\qdom}{\mathcal{Q}}
{\mathcal{Q}}
\end_inset


\end_layout

\begin_layout Section
Introduction
\end_layout

\begin_layout Standard
A central question in many probabilistic clustering problems is how many
 distinct clusters are present in a particular dataset.
 In all but the simplest problems, any attempt to answer this question may
 be strongly determined by the criterion used.
 One common approach, based in the Bayesian tradition, addresses the problem
 with a generative model: out of a population of unobserved latent clusters,
 some finite number are randomly chosen to be present in the actual data
 at hand.
 The identity and number of these clusters can then be estimated – with
 attendant uncertainty estimates – using the tools of Bayesian posterior
 inference.
 For example, one might estimate the 
\begin_inset Quotes eld
\end_inset

number of distinct clusters present
\begin_inset Quotes erd
\end_inset

 by its posterior expectation.
 Such a generative model is called a 
\begin_inset Quotes eld
\end_inset

Bayesian non-parametric
\begin_inset Quotes erd
\end_inset

 (BNP) model when the number of latent clusters is infinite, though, naturally,
 even in non-parametric models only a finite number of clusters can actually
 be observed in any particular dataset.
 
\end_layout

\begin_layout Standard
As with any Bayesian model, this approach requires the specification of
 a prior and a likelihood.
 In this case, the likelihood describes the dispersion of data within a
 particular cluster, and the prior determines both the distribution of cluster
 shapes and sizes as well as the process that determines how many clusters
 are present.
 In general, different choices of the prior and likelihood would give different
 answers to the question 
\begin_inset Quotes eld
\end_inset

how many distinct clusters are present?
\begin_inset Quotes erd
\end_inset

 For example, if the prior does not somehow prefer fewer, larger clusters,
 then there is nothing that inherently prevents such an approach from inferring
 that each datapoint is in its own cluster.
 However, one still hopes that a broad range of reasonable choices of prior
 and likelihood will come to similar conclusions.
 Consequently, it is important, in practice, to measure the sensitivity
 of the inferred number of clusters present to the prior and likelihood
 specification.
 Furthermore, these sensitivity measures should work with the kinds of inference
 tools that are used in practice, operate relatively automatically without
 re-fitting the model many times, and measure sensitivity not only to alternativ
e hyperparameters but also to alternative functional forms of the prior
 and likelihood.
 
\end_layout

\begin_layout Standard
To address these needs, we develop fast, automatic measures of the sensitivity
 of variational Bayes (VB) approximations to perturbations of functional
 forms in a putative model.
 As a motivating application, we apply our techniques to estimate the sensitivit
y of BNP posteriors to the functional form of a particular BNP prior known
 as the stick-breaking prior.
 As we will see below, stick-breaking priors provide a strong motivation
 to quantify functional perturbations.
 A typical choice of a stick breaking prior is specified with only a single
 real-valued hyperparameter and also a potentially informative distributional
 assumption, the form of a stick breaking prior can substantially inform
 the number of clusters inferred to be present in a particular dataset,
 and it is arguably difficult for ordinary practitioners to form meaningful
 subjective beliefs about the abstract form of the stick breaking prior.
\end_layout

\begin_layout Standard
We begin by deriving a general result for the sensitivity of VB optima to
 function-valued perturbations, as well as several useful specializations.
 We then describe a VB approximation to a BNP model with a stick-breaking
 prior and derive the sensitivity of the approximate number of inferred
 clusters to the choice of the stick breaking prior.
 We then apply our methods to a real dataset, comparing our results to the
 much more expensive process of re-fitting the model.
\end_layout

\begin_layout Section
Functional sensitivity
\end_layout

\begin_layout Subsection
Variational Bayes
\end_layout

\begin_layout Standard
Suppose we observe data, 
\begin_inset Formula $X$
\end_inset

, and a parameter 
\begin_inset Formula $\theta\in\thetadom\subseteq\mathbb{R}^{K}$
\end_inset

, which parameterizes a generative model for 
\begin_inset Formula $X$
\end_inset

, denoted 
\begin_inset Formula $p\left(X\vert\theta\right)$
\end_inset

.
 Here, 
\begin_inset Formula $K$
\end_inset

 may be finite or, as in the case of BNP, countably infinite.
 Defining a base prior, 
\begin_inset Formula $\baseprior$
\end_inset

, with respect to a dominating measure 
\begin_inset Formula $\lambda$
\end_inset

 on 
\begin_inset Formula $\thetadom$
\end_inset

, we can calculate the posterior distribution on 
\begin_inset Formula $\theta$
\end_inset

 given 
\begin_inset Formula $X$
\end_inset

:
\begin_inset Formula 
\begin{align}
\basepost & :=\frac{p\left(X\vert\theta\right)\baseprior}{\int p\left(X\vert\theta'\right)\baseprior[\theta']\lambda\left(d\theta'\right)}=\frac{p\left(X\vert\theta\right)\baseprior}{p\left(X\right)}.\label{eq:posterior_defn}
\end{align}

\end_inset

Suppose we are interested in calculating a posterior expectation of some
 function of interest, say 
\begin_inset Formula $\mbe_{\basepost}\left[\targetfun\right]$
\end_inset

.
 In many cases (such as most clustering problems), 
\begin_inset Formula $\basepost$
\end_inset

 is intractable and must be approximated.
 One popular approximation technique, known as variational Bayes (VB), approxima
tes 
\begin_inset Formula $\basepost$
\end_inset

 in KL divergence with a distribution in some convenient sub-class of all
 possible distributions.
 Let 
\begin_inset Formula $\qdom$
\end_inset

 be a set of distributions parameterized by the real-valued parameter 
\begin_inset Formula $\eta\in\etadom\subseteq\mathbb{R}^{K_{q}}$
\end_inset

:
\begin_inset Formula 
\begin{align*}
\qdom & :=\left\{ q:q=\qpost[][\eta]\textrm{ for }\eta\in\etadom\right\} .
\end{align*}

\end_inset

For example, 
\begin_inset Formula $\qdom$
\end_inset

 might be a convenient exponential family such as a factorizing normal distribut
ion, in which case 
\begin_inset Formula $\eta$
\end_inset

 would parameterize the mean and covariance of 
\begin_inset Formula $\qpost[][\eta]$
\end_inset

.
 The optimal parameter, 
\begin_inset Formula $\etaopt$
\end_inset

, is the parameter that minimizes the KL divergence between 
\begin_inset Formula $\qpost[][\eta]$
\end_inset

 and 
\begin_inset Formula $\basepost$
\end_inset

:
\begin_inset Formula 
\begin{align}
KL\left(\qpost[][\eta]||\basepost\right) & :=\mbe_{\qpost[][\eta]}\left[\log\qpost[][\eta]-\log\basepost\right]\nonumber \\
\etaopt & :=\underset{\eta\in\etadom}{\mathrm{argmin}}KL\left(\qpost[][\eta]||\basepost\right).\label{eq:kl_opt}
\end{align}

\end_inset

The distribution 
\begin_inset Formula $\qpost$
\end_inset

 is then the closest distribution in 
\begin_inset Formula $\qdom$
\end_inset

 to 
\begin_inset Formula $\basepost$
\end_inset

 as measured by KL divergence, and is called the 
\begin_inset Quotes eld
\end_inset

variational approximation
\begin_inset Quotes erd
\end_inset

 to 
\begin_inset Formula $\basepost$
\end_inset

.
 Noting that
\begin_inset Formula 
\begin{align*}
\mbe_{\qpost[][\eta]}\left[\log\basepost\right] & =\mbe_{\qpost[][\eta]}\left[\log p\left(X\vert\theta\right)+\log\baseprior\right]-\log p\left(X\right),
\end{align*}

\end_inset

we see that the intractable normalizing constant of 
\begin_inset Formula $\basepost$
\end_inset

 can be neglected when optimizing to find 
\begin_inset Formula $\etaopt$
\end_inset

.
 We then hope that 
\begin_inset Formula $\qdom$
\end_inset

 is sufficiently expressive that
\begin_inset Formula 
\begin{align*}
\mbe_{\qpost}\left[\targetfun\right] & \approx\mbe_{\basepost}\left[\targetfun\right].
\end{align*}

\end_inset


\end_layout

\begin_layout Subsection
Prior sensitivity
\end_layout

\begin_layout Standard
The posterior expectation 
\begin_inset Formula $\mbe_{\basepost}\left[\targetfun\right]$
\end_inset

 depends on the choice of prior, 
\begin_inset Formula $\baseprior$
\end_inset

, through the definition of the posterior in 
\begin_inset CommandInset ref
LatexCommand formatted
reference "eq:posterior_defn"
plural "false"
caps "false"
noprefix "false"

\end_inset

.
 Similarly, the VB approximate posterior expectation 
\begin_inset Formula $\mbe_{\qpost}\left[\targetfun\right]$
\end_inset

 depends on the choice of 
\begin_inset Formula $\baseprior$
\end_inset

 through its occurrence in the objective of the optimization problem 
\begin_inset CommandInset ref
LatexCommand formatted
reference "eq:kl_opt"
plural "false"
caps "false"
noprefix "false"

\end_inset

.
 In order to quantify this dependence, consider a bounded 
\begin_inset Formula $\lambda$
\end_inset

-integrable perturbation 
\begin_inset Formula $\priorpert$
\end_inset

 (which may be positive or negative), take some 
\begin_inset Formula $\epsilon\ge0$
\end_inset

, and define the 
\begin_inset Quotes eld
\end_inset

contamination prior
\begin_inset Quotes erd
\end_inset

:
\begin_inset Formula 
\begin{align*}
\contamprior & :=\frac{\baseprior+\epsilon\priorpert}{\int\left(\baseprior[\theta']+\epsilon\priorpert[\theta']\right)\lambda\left(d\theta'\right)}.
\end{align*}

\end_inset

As long as 
\begin_inset Formula $\epsilon$
\end_inset

 is chosen so that 
\begin_inset Formula $\baseprior+\epsilon\priorpert\ge0$
\end_inset

, the distribution 
\begin_inset Formula $\contamprior$
\end_inset

 is a well-defined distribution.
 Because 
\begin_inset Formula $\priorpert$
\end_inset

 is bounded, there always exists an open interval for 
\begin_inset Formula $\epsilon$
\end_inset

 such that 
\begin_inset Formula $\contamprior$
\end_inset

 is well-defined.
 By using 
\begin_inset Formula $\contamprior$
\end_inset

 as our prior, for any 
\begin_inset Formula $\epsilon$
\end_inset

 we can define the 
\begin_inset Quotes eld
\end_inset

contamination posterior
\begin_inset Quotes erd
\end_inset

:
\begin_inset Formula 
\begin{align*}
\post & :=\frac{p\left(X\vert\theta\right)\contamprior}{\int p\left(X\vert\theta'\right)\contamprior[\theta']\lambda\left(d\theta\right)}.
\end{align*}

\end_inset


\end_layout

\begin_layout Standard
GUSTAFSON showed some damn shit
\end_layout

\end_body
\end_document
