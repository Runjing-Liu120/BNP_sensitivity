
\paragraph{General hyperparameter sensitivity.}
%
We with to approximate the sensitity of $g(\etaopt)$ to perturbations of the
value of $\alpha$ and to the functional form of $p_{0k}$.  To do this, we will
call on a general result for the sensitivity of VB optima to vectors of
real-valued hyperparameters.  Suppose the exact posterior is parameterized by a
real-valued hyperparameter $\epsilon$, i.e., the posterior is given by
$p\left(\theta, z \vert y, \epsilon\right)$. In the present work, $\epsilon$
will parameterize perturbations to the prior, as we will describe in more detail
shortly.  Then the optimal variational approximation is also a function of
$\epsilon$ through the minization of the KL divergence.  We can define
%
\begin{align}
KL\left(\eta_\theta, \epsilon\right) &:=
    KL\left(q(\theta, z \vert \eta_\theta, \etazopt\left(\eta_\theta\right))
    \big\| p(\theta, z | y, \epsilon) \right) \nonumber \\
\etathetaopt\left(\epsilon\right) &=
    \argmin_{\eta_\theta} KL\left(\eta_\theta, \epsilon\right)
    \label{eq:pert_kl_objective}
\end{align}
%
In general, the dependence of $\etathetaopt\left(\epsilon\right)$ on $\epsilon$ is
complex and non-linear, but we may approximate it with a first-order Taylor
series.
Under mild regularity conditions that are satisfied in the present case,
\citet[Theorem 2]{giordano:2017:covariances} gives a closed form
expression for this Taylor series.
Without loss of generality, let $\epsilon=0$ represent the unperturbed
posterior, so that $p\left(\theta, z \vert y, \epsilon=0\right) = p\left(\theta,
z \vert y \right)$.
Define
$H := \partial^2 KL(\eta_\theta, \epsilon) /
    \partial \eta_\theta \partial \eta_\theta^T
    \atzero$ and
$f_\eta := \partial^2
    \QExpect \left[ \log p\left(y, \theta, z \vert \epsilon \right) \right]
    / \partial \eta_\theta \partial \epsilon^T
    \atzero$.
Then
%
\begin{align}
\etathetaopt(\epsilon)  -  \etathetaopt(0) &\approx
\frac{d \etathetaopt(\epsilon)}{d\epsilon^T}\Big|_{\epsilon=0} \epsilon =
- H^{-1} f_\eta \epsilon
\label{eq:our_approximation}
\end{align}
%
Note that $H$ and $f_\eta$ can be easily evaluated using automatic
differentiation without any need to re-optimize for different $\epsilon$
\citep{maclaurin:2015:autograd} (see \prettyref{app:sensitivity}).  Furthermore,
the Hessian $H$ need be factorized (e.g. with a Cholesky decomposition) or
inverted only once and then re-used to approximate $\etathetaopt(\epsilon)$ for
many different perturbations.

\paragraph{Allowing nonlinearity in computationally easy steps.}
%
Note that the complete mapping
$\epsilon \mapsto \Expect_{\qopt} \left[ \#\{\text{distinct clusters}\} \right]$
is, in general, composed of many highly non-linear steps:
%
\begin{align*}
\epsilon \mapsto
\etathetaopt(\epsilon) \mapsto
\etazopt\left(\etathetaopt\right) \mapsto
\textrm{Draws from }z \sim q(z \vert \etazopt) \mapsto
\Expect_z\left[\sum_{k=1}^K \prod_{n=1}^N \mathbb{I}\{z_n = k\}\right].
\end{align*}
%
However, only the first step, $\epsilon \mapsto \etathetaopt(\epsilon)$ is
computationally intensive (re-solving the optimization problem
\prettyref{eq:pert_kl_objective} with a new $\epsilon$), and it is precisely
this first step which we approximate linearly using
\prettyref{eq:our_approximation}, i.e., with $\epsilon \mapsto \etathetaopt(0) -
H^{-1} f_\eta \epsilon$. Consequently, our approximations retain the
nonlinearity in the mapping $\etathetaopt \mapsto \Expect_z\left[\sum_{k=1}^K
\prod_{n=1}^N \mathbb{I}\{z_n = k\}\right]$.

Furthermore, we attempt to improve the linearity of the dependence of
$\etathetaopt$ on $\epsilon$ by using an unconstrained parameterization for
$\eta_\theta$ as in \citet{stan-manual:2015} and \citet{kucukelbir:2015:advi}.
