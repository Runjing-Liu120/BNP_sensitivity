%
\paragraph{Variational approximation.} It is difficult to calculate the
posterior $p\left(\nu, \mu, \Sigma, z \vert y\right)$, both because the
normalizing constant is intractable and because there are an infinite number of
latent clusters in a true BNP representation. In order to perform approximate
inference, we use a truncated VB approximation using $K=30$
clusters\citep{blei:2006:dirichletbnp}. For compactness of notation, let $\theta =
\left(\nu, \mu, \Sigma\right)$ denote the collection of ``global'' parameters,
i.e., parameters whose values affect the data-generating process of every
observation $y_n$.   Let $\delta\left(\cdot\right)$ denote a delta function. We
define a class of approximating distributions for VB as
%
\begin{align*}
\vbfamily := \Big\{ q:
q(\theta, z) &=
\left(\prod_{k=1}^{K}q\left(\nu_{k}\right)\delta\left(\mu_{k}\right)
    \delta\left(\Sigma_{k}\right)\right)
    \left(\prod_{n=1}^{150}q\left(z_{n}\right) \right)\textrm{, where } \\
q\left(\nu_{k}\right) &= \mathrm{Lognormal}\left(\nu_k\right) \textrm{ and }
q\left(z_n\right) = \mathrm{Categorical}\left(z_n\right)
\Big\}.
\end{align*}
%
The family $\vbfamily$ is parameterized by a finite-dimensional vector containing
the locations of the delta functions and the parameters for the lognormal
distributions,
%
\footnote{We use the lognormal distribution rather than the conjugate Beta
distribution because the lognormal makes numerical integration easier when
re-optimizing using non-conjugate $p_{0k}$.  Were one to simply rely on our
sensitivity measures and not re-optimize, there would be no need for numerical
integration, and the more convenient Beta variational approximation could be
used.}
%
which we denote by $\eta_\theta$, and the parameters for the
multinomial distributions, which we denote by $\eta_z$. We write the combined
parameters as $\eta=\left(\eta_\theta, \eta_z\right)$. That is, $\eta$ is
defined such that
%
$\vbfamily =
    \left\{q: q\left(\theta, z\right) =
            q\left(\theta, z \vert \eta \right) =
            q\left(\theta \vert \eta_\theta \right)
            q\left(z \vert \eta_z \right)
    \right\}.$
%
The variational approximation is then given by
%
%\begin{align*}
$
\etaopt = \argmin_{\eta} KL\left(
    q(\theta, z \vert \eta) \big\| p(\theta, z | y)
    \right). %\label{eq:kl_objective}
$
%\end{align*}

It will be important later to note that it is easy to calculate the optimal
$\eta_z$ for a given $\eta_\theta$ because the model is conditionally conjugate,
i.e., $p\left(z \vert \theta, y\right)$ is categorical, and so is $q\left(z
\vert \eta_z\right)$.  Specifically, there exists an easily-calculated, closed
form for
%
%\begin{align}
$
\etazopt\left(\eta_\theta\right) = \argmin_{\eta_z}
    KL\left(
    q(\theta \vert \eta_\theta) q( z \vert \eta_z)
        \big\| p(\theta, z | y)
    \right).
$
%\label{eq:z_update}
%\end{align}
%

%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%


\paragraph{Target posterior quantity.}

We are interested in the inferred number of clusters present in the observed
data, which can be expressed as an expection with respect to
$q\left(z \vert \eta_z \right)$, and therefore as a function of
$\etathetaopt$ via the relation $\etazopt(\eta_\theta)$:
%
\begin{align}
g(\etathetaopt) &:=
\Expect_{q(\theta, z \vert \etaopt)} \left[ \#\{\text{distinct clusters}\} \right]  =
\Expect_{q(z \vert \etazopt(\eta_\theta))} \left[
    \sum_{k=1}^K \left(1 - \prod_{n=1}^N \mathbb{I}\{z_n \ne k\} \right) \right].
    \label{eq:expected_num_clusters}
\end{align}
%
For a given optimal set of global variational parameters, $g(\etathetaopt)$ can
be computed with Monte-Carlo draws of the cluster indicators, $z \iid q\left(z
\vert \etazopt(\etathetaopt)\right)$.  We will denote Monte-Carlo expectations
by $\Expecthat[\cdot]$.
