
TODO (Ryan): rewrite and shorten intro

% A central question in many probabilistic clustering problems is how many
% distinct clusters are present in a particular dataset. In all but the simplest
% problems, any attempt to answer this question may be strongly determined by the
% criterion used. One common approach, based in the Bayesian tradition, addresses
% the problem with a generative model: out of a population of unobserved latent
% clusters, some finite number are randomly chosen to be present in the actual
% data at hand. The identity and number of these clusters can then be estimated –
% with attendant uncertainty estimates – using the tools of Bayesian posterior
% inference. For example, one might estimate the ``number of distinct clusters
% present'' by its posterior expectation. Such a generative model is called a
% ``Bayesian non-parametric'' (BNP) model when the number of latent clusters is
% infinite, though, naturally, even in non-parametric models only a finite number
% of clusters can actually be observed in any particular dataset.
%
% As with any Bayesian model, this approach requires the specification of a prior
% and a likelihood. In this case, the likelihood describes the dispersion of data
% within a particular cluster, and the prior determines both the distribution of
% cluster shapes and sizes as well as the process that determines how many
% clusters are present. In general, different choices of the prior and likelihood
% would give different answers to the question “how many distinct clusters are
% present?” For example, if the prior does not somehow prefer fewer, larger
% clusters, then there is nothing that inherently prevents such an approach from
% inferring that each datapoint is in its own cluster. However, one still hopes
% that a broad range of reasonable choices of prior and likelihood will come to
% similar conclusions. Consequently it is important, in practice, to measure the
% sensitivity of the inferred number of clusters present to the prior and
% likelihood specification. Furthermore, these sensitivity measures should work
% with the kinds of inference tools that are used in practice, operate relatively
% automatically without re-fitting the model many times, and measure sensitivity
% not only to alternative hyperparameters but also to alternative functional forms
% of the prior and likelihood.
%
% To address these needs, we develop fast, automatic measures of the sensitivity
% of variational Bayes (VB) approximations to perturbations of functional forms in
% a putative model. As a motivating application, we apply our techniques to
% estimate the sensitivity of BNP posteriors to the functional form of a
% particular BNP prior known as the stick-breaking prior. Stick-breaking priors
% provide a strong motivation to quantify functional perturbations. A typical
% choice of a stick breaking prior is specified with only a single real-valued
% hyperparameter and also a potentially informative distributional assumption, the
% form of a stick breaking prior can substantially inform the number of clusters
% inferred to be present in a particular dataset, and it is arguably difficult for
% ordinary practitioners to form meaningful subjective beliefs about the abstract
% form of the stick breaking prior.

We begin by deriving a general result for the sensitivity of VB optima to
function-valued perturbations, as well as several useful specializations. We
then describe a VB approximation to a BNP model with a stick-breaking prior and
derive the sensitivity of the approximate number of inferred clusters to the
choice of the stick breaking prior. We then apply our methods to cluster the
Iris \citep{iris_data_anderson, iris_data_fisher} dataset, comparing our results
to the much more expensive process of re-fitting the model.
