We use the iris dataset \citep{iris_data_anderson,
iris_data_fisher}, which contains 150 observations of three different types of
iris flowers. We use measurements of their sepal length, sepal width, petal
length, and petal width to cluster the data with the goal of recovering the
three species. Let $y_{n}\in \mathbb{R}^4$ be these four measurements for flower
$n$.

Let $z_n$ denote the cluster (i.e. the species, which we treat as unknown) to
which flower $n$ belongs. Each cluster has mean $\mu_k\in \mathbb{R}^4$ and
covariance $\Sigma_k \in \mathbb{R}^{4\times 4}$. Our data generating process is
then
%
\begin{align}
	y_n | z_n \sim \mathcal{N}\Big(\sum_{k=1}^\infty \mathbb{I}\{z_n = k\} \mu_k \;,
              \; \sum_{k=1}^\infty \mathbb{I}\{z_n = k\} \Sigma_k\Big),
	\quad n = 1, ..., N
\end{align}
%
Note that we allow for an infinite number of clusters. The next section
describes the Bayesian non-parametric prior that makes this possible.

\subsection{The Prior}
We place priors on $\mu_k$, $\Sigma_k$, and $z_n$.

We use a Dirichlet stick breaking process \citep{ferguson:1973:bayesian,
sethuraman:1994:constructivedp} as our prior on the cluster components weights
$\pi$, and draw $z_n$ from a multinomial with said weights:
%
\begin{align}
	\pi &\sim \text{GEM}(\alpha) \label{eq:GEM} \\
	 z_n &\sim \text{Multinomial}(\pi), \quad n = 1..., 150
\end{align}
%
We can write out equation~\ref{eq:GEM} more explicitly as,
%
\begin{align}
  \nu_k | \alpha &\sim Beta(1, \alpha) \quad k = 1, .., \infty \label{eq:beta_sticks}\\
  \pi_k | \nu &= \nu_k \prod_{j=1}^{k-1} (1 - \nu_j) \label{eq:stick_breaking}
\end{align}
%
so posterior inference on the cluster weights $\pi_k$, amounts to doing
posterior inference on the sticks $\nu_k$'s.

The other parameters have priors:
\begin{align}
	\mu_k &\sim \mathcal{N}(0, \sigma^2 I_{4\times 4}), \quad k = 1, 2, 3 ... \\
	\Sigma_k &\sim \text{Inv.Wishart}(d, V), \quad k = 1, 2, 3 ...
\end{align}
