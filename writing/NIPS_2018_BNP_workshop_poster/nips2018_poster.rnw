\documentclass[a0,plainsections,30pt]{sciposter}

\usepackage{graphicx}
% \usepackage{subcaption}
\usepackage{caption}

\usepackage{epstopdf, graphicx}
\usepackage{booktabs,dcolumn}
\usepackage{amsmath}
\usepackage{amsthm}
\usepackage{amssymb}
\usepackage{bbm}
\usepackage{hhline}
\usepackage{multicol}
\setlength{\columnseprule}{0pt}

\usepackage{boxedminipage}

\newcommand{\widgraph}[2]{\includegraphics[keepaspectratio,width=#1]{#2}}

\newcommand{\Rbb}{\mathbb{R}}
\newcommand{\Expect}{\mathbb{E}}
\newcommand{\Expecthat}{\hat{\mathbb{E}}}
\newcommand{\Var}{\text{Var}}
\newcommand{\Cov}{\text{Cov}}
\newcommand{\vbfamily}{\mathcal{Q}}
\newcommand{\etaopt}{\eta^{*}}
\newcommand{\etazopt}{\eta_z^{*}}
\newcommand{\etathetaopt}{\eta_\theta^{*}}
%\newcommand{\qopt}{q^{*}}
\newcommand{\targethat}{\hat{g}}
\newcommand{\QExpect}
{\Expect_{q\left(\theta, z \vert \eta_\theta, \etazopt(\eta_\theta)\right)}}
\newcommand{\atzero}{\Big\rvert_{\eta_\theta = \etathetaopt, \epsilon = 0}}
\newcommand{\etathetalin}{\eta_\theta^{LIN}}

\usepackage[framemethod=TikZ, xcolor=RGB]{mdframed}

\definecolor{mydarkblue}{rgb}{0,.06,.5}
\definecolor{mydarkred}{rgb}{.5,0,.1}
\definecolor{myroyalblue}{rgb}{0,.1,.8}

\usepackage{sectsty}
\sectionfont{\color{mydarkblue}\centering\LARGE\bf}

\mdfdefinestyle{MyFrame}{%
    linecolor=mydarkblue,
    outerlinewidth=2pt,
    roundcorner=20pt,
    innertopmargin=10pt,
    innerbottommargin=10pt,
    innerrightmargin=10pt,
    innerleftmargin=10pt,
    backgroundcolor=blue!10}


\title{\textcolor{mydarkblue}{Evaluating Sensitivity to the Stick Breaking Prior in Bayesian Nonparametrics
}}
% \usepackage{authblk}
% \author[1]{Author A}
% \author[1]{Author B}
% \author[1]{Author C}
% \author[1]{Author D}
% \author[2]{Author E}
% \affil[1]{Department of Computer Science, \LaTeX\ University}
% \affil[2]{Department of Mechanical Engineering, \LaTeX\ University}

\author{Ryan Giordano\textsuperscript{1*} \quad 
Runjing Liu\textsuperscript{1*} \quad 
Michael I. Jordan\textsuperscript{1} \quad 
Tamara Broderick\textsuperscript{2} \\
{\large\normalfont\textsuperscript{*} These authors contributed equally}\\
 {\large\normalfont\textsuperscript{1} Department of Statistics, UC Berkeley \quad \textsuperscript{2} Department of EECS, MIT}
 }

\leftlogo[1]{images/logo_left.png}
\rightlogo[1]{images/logo_right2.png}

% Set the color used for the section headings here
\definecolor{SectionCol}{rgb}{0,.06,.5}
\definecolor{lightblue}{rgb}{0.8,0.8,1}

% Set some fbox commands line width and the color we use in the f boxes
\setlength{\fboxrule}{.09cm}
\definecolor{boxcolor}{rgb}{1,1,1}
\definecolor{innerboxcolor}{rgb}{.9,.94,.98}

% Math macros
\newcommand{\eq}[1]{Eq.~(\ref{eq:#1})}

\newcommand{\kl}{\textrm{KL}}
\newcommand{\mbe}{\mathbb{E}}
\newcommand{\mbeq}{\mathbb{E}_{q}}
\newcommand{\var}{\textrm{Var}}
\newcommand{\cov}{\textrm{Cov}}
\newcommand{\iid}{\stackrel{iid}{\sim}}
\newcommand{\indep}{\stackrel{indep}{\sim}}
\newcommand{\gauss}{\mathcal{N}} % Gaussian distribution

\DeclareMathOperator*{\argmin}{arg\,min}

% \newcommand{\npp}{\tilde{\eta}} % natural parameter for the p distribution
% \newcommand{\npq}{\eta} % natural parameter for the q distribution
% \newcommand{\mpp}{\tilde{m}} % mean parameter for the p distribution
% \newcommand{\mpq}{m} % mean parameter for the q distribution
% \newcommand{\mpopt}{m^*} % mean parameter for the q^* distribution
% \newcommand{\npopt}{\eta^*} % mean parameter for the q^* distribution
% \newcommand{\truecov}{\Sigma} % True posterior covariance
% \newcommand{\lrcov}{\hat{\Sigma}} % LR cov estimate
% \newcommand{\vbcov}{V} % Variational posterior covariance
% \newcommand{\constant}{C} % A constant

% my added commands
\usepackage{etoolbox}
\BeforeBeginEnvironment{figure}{\vskip-2ex}
\AfterEndEnvironment{figure}{\vskip-1ex}

% Fiddle with the margin
\addtolength{\topmargin}{-0.5in}
% \addtolength{\topmargin}{-0.875in}
\addtolength{\textheight}{1in}
\begin{document}
\conference{NIPS 2017}


\setlength{\parskip}{0.25em}

\maketitle

\vspace{-1in}


<<initialization, echo=FALSE, message=FALSE, results='asis'>>=
library(tidyverse)
source("./R_graphs/Initialize.R")

data_path <- file.path("./R_graphs/data/")
source("./R_graphs/LoadData.R")
source("./R_graphs/LoadNumbers.R")

# Turn off caching if you need to regenerate any of the R code.
opts_chunk$set(cache=TRUE)
@




%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% FIRST COLUMN
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%\columnbreak

\begin{minipage}[t]{0.45\textwidth}

\begin{mdframed}[style=MyFrame]
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%
\section*{Overview}
\vspace{-0.3in}
%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{itemize}

\item Data analysts are often interested to discover the number of
clusters in a dataset. 
\item Bayesian nonparametrics (BNP) allows discovery of cluster
cardinality for a fixed dataset size. 
\item Variational Bayes (VB) is a way to approximate the BNP posterior. 
\item \textbf{Problem}: \textbf{how sensitive is the resulting
VB approximation of cluster cardinality to BNP model choices}? Evaluating
for multiple model choices would be expensive. 
\item \textbf{We propose}: \textbf{a linear approximation} to efficiently
estimate BNP sensitivity \textbf{from a single run of VB} (to avoid
expensive refitting). 
\item Namely, we evaluate sensitivity of cluster cardinality to the Dirichlet process
stick-breaking parameter and functional form. 

\end{itemize}
\end{mdframed}
\vspace{-0.7in}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%
\section*{Model and inference }
\vspace{-0.3in}
%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

The \textbf{Dirichlet process} is a popular Bayesian nonparametric
(BNP) model often used to model clusters. 

\begin{figure}[!h]
\centering
\includegraphics[width = 0.95\textwidth]{./images/DP_stick_breaking.png}
\caption{The Dirichlet stick-breaking process. We start with a stick of
length 1, and recursively break off proportions of length $\nu_1$, $\nu_2$, $\nu_3$, ...}
\setlength{\textfloatsep}{-10pt}
\end{figure}

%%% iris figure
% \begin{figure}[!h]
% \centering
% \includegraphics[width = 0.7\textwidth]{./images/iris_data.png}
% 
% \caption{The iris data projected onto the first two principal components. Color corresponds to the true iris species. }
% 
% \setlength{\textfloatsep}{-10pt}
% \end{figure}

\vspace{-0.3in}

We approximate the true posterior using \textbf{Variational Bayes} (VB). Note that: 

\begin{itemize}
\item $\eta_\theta$: the variational parameters for the global parameters (the stick-breaking proportions, the cluster centroids, the cluster covariances). 
\item $\eta_z$: the variational parameters for the categorical distributions on the 
cluster belongings $z$. 
\end{itemize}

Note that by choosing a conditionally conjugate model for the cluster belongings, for a given $\eta_\theta$ we can solve in closed form the optimal $\eta_z$.

\begin{mdframed}[style=MyFrame]
- We wish to know how varying the stick-breaking proportion distribution $\nu_k \sim \text{Beta}(1, \alpha)$ will affect the inferred number of clusters. 

- While VB is fast, refitting for multiple choices of the DP prior would be
computationally prohibitive. {\bf Hence, propose a linear approximation, derived from local sensitity measures. }
\end{mdframed}

% Our objective is then 
% \vspace{-0.2in}
% \begin{align}
% \etathetaopt =
% \argmin_{\eta_\theta} KL\left(
%     q(\theta, z \vert \eta_\theta, \etazopt(\eta_\theta) \big\| p(\theta, z | y)
%     \right)
%     \label{eq:kl_objective}
% \end{align}

% \vspace{-0.7in}

% %%%%%%%%%%%%%%%%%%
% \subsection*{Posterior expectations of interest: }
% \vspace{-0.3in}
% %%%%%%%%%%%%%%%%%%
% We consider two possible posterior quantities:
% 
% \begin{enumerate}
% \item An {\bf in-sample quantity}, the expected number of clusters in the given iris dataset.
% % \vspace{-0.3in}
% % \begin{align}
% % \Expect_{q(z \vert \etazopt(\etathetaopt))} \left[ \#\{\text{distinct clusters}\} \right]
% % \label{eq:expected_num_clusters_thresh}
% % \end{align}
% 
% \item A {\bf posterior predictive quantity}, or the number of clusters we expect see in a \textit{new} dataset of $N$ iris flowers, given our posterior knowledge.
% % \begin{align}
% % \Expect_{q(\nu \vert \etathetaopt)} 
% % \left[\#\{\substack{\text{distinct clusters}\\\text{in new dataset}}\} \right] 
% %     \label{eq:expected_num_clusters_pred_thresh}
% % \end{align}
% 
% \end{enumerate}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%
\vspace{-0.6in}
\section*{Hyperparameter sensitivity}
\vspace{-0.3in}
%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{itemize}

\item Let $\epsilon$ be a real-valued hyperparameter for the stick-breaking distribution
(e.\ g.\ , this could be the $\alpha$ concentration parameter). 

\item The VB parameters $(\eta_\theta, \eta_z)$ depend on $\epsilon$. 

\item The expected number of clusters present in the {\itshape current} iris dataset is a composition of the functions, 
\vspace{-0.1in}
\begin{align}
\epsilon \mapsto
\etathetaopt(\epsilon) \mapsto
\etazopt\left(\etathetaopt\right) \mapsto
\Expect_{q(z | \etazopt)} \left[ \#\{\text{distinct clusters}\} \right].
\label{eq:maps_in_sample}
\end{align}
\vspace{-0.1in}
Similarly for the predictive quantity, the expected number of clusters present in a {\itshape new} dataset: 
\vspace{-0.1in}
\begin{align}
\epsilon \mapsto
\etathetaopt(\epsilon) \mapsto
\Expect_{q(\nu \vert \etathetaopt)} 
\left[\#\{\substack{\text{distinct clusters}\\\text{in new dataset}}\} \right].
\label{eq:maps_predictive}
\end{align}
\end{itemize}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%
\vspace{-0.9in}
\subsection*{Linear approximation}
\vspace{-0.2in}
%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{itemize}
\item WOLOG, let $\epsilon=0$ represent the original posterior, and we approximate
\begin{align}
\etathetaopt(\epsilon)  &\approx  \etathetaopt(0) + 
\frac{d \etathetaopt(\epsilon)}{d\epsilon^T}\Big|_{\epsilon=0} \epsilon
\label{eq:linear_approx}
\end{align}

\item Evaluation of the derivative can be done efficiently using formulas from \cite{giordano:2017:covariances} and auto-differentiation tools \cite{maclaurin:2015:autograd}.
 
\item We only use a linear approximation for the map $\epsilon \mapsto \etathetaopt(\epsilon)$. {We retain the non-linearities in the other maps of (\ref{eq:maps_in_sample}) and (\ref{eq:maps_predictive}). }

% as $\frac{d \etathetaopt(\epsilon)}{d\epsilon^T}\Big|_{\epsilon=0} = H^{-1} f_\eta$ 

\end{itemize}
% where 
% \begin{align}
% H := \frac{\partial^2 KL(\eta_\theta, \epsilon) }{
%     \partial \eta_\theta \partial \eta_\theta^T}
%     \atzero
% \qquad 
% f_\eta := \frac{\partial^2
%     \QExpect \left[ \log p\left(y, \theta, z \vert \epsilon \right) \right]}{\partial \eta_\theta \partial \epsilon}
%     \atzero.
%     \label{eq:sensitivity_formulae}
% \end{align}
% where $KL(\eta_\theta, \epsilon)$ is the KL objective in Equation~\ref{eq:kl_objective}.
% \end{itemize}

\end{minipage}
\hfill \vrule \hfill
\begin{minipage}[t]{0.45\textwidth}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% SECOND COLUMN 
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%
\section*{Results}
\vspace{-0.3in}
%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
We infer the number of distinct species in the iris
dataset \cite{iris_data_anderson}, and evaluate its sensitivity to the BNP prior.  
\vspace{0.1in}

{\bf \large Sensitivity to $\alpha$.}
We evaluate the dependence of the expected number of clusters to the stick-breaking concentration parameter $\alpha$. 

% \begin{align*}
% f^\alpha_\eta := \frac{\partial^2
%   \QExpect
%   \left[ \log p\left(\nu \vert \alpha \right) \right]}
% { \partial \eta_\theta \partial \alpha^T }
% \Big\rvert_{\eta_\theta = \etathetaopt, \alpha = \alpha_0},
% \end{align*}

% \vspace{-0.4in}

<<setup, include=FALSE, cache=FALSE>>=
knitr_debug <- FALSE # Set to true to see error output
cache_knitr <- FALSE # Set to true to cache knitr output for this analysis.
source("./Rscripts/initialize.R", echo=FALSE)
source("./Rscripts/plot_sens_results.R")
@

\begin{figure}
\centering
<<param_sens_plot_thresh_0, cache=cache_knitr, fig.show='hold'>>=
source("./Rscripts/appendix_parametric_sens_results_thresh0.R",
      echo=knitr_debug, print.eval=TRUE)
@
\caption{Comparison of in-sample (top) and predictive (bottom) expected number of clusters computed by re-optimizing versus the linear approximation. 
The blue vertical line indicates the location of $\alpha_0$}
\end{figure}

{\bf \large Sensitivity to functional perturbations. }
We consider a multiplicative perturbation $\phi$ to
the original beta distribution $p_0$ for the stick-breaking proportions:
\vspace{-0.2in}
\begin{align}
\label{eq:expon_perturb}
	p_c(\nu_k \vert \delta, \phi) :=
  \frac{p_{0}(\nu_k)\phi(\nu_k)^\delta}
       {\int_0^1 p_0(\nu_k')\phi(\nu')^\delta d\nu_k'}.
\end{align}
We evaluate the effect of $\delta$:
\vspace{0.1in}
\begin{figure}
\centering
<<fig_cap2, cache=cache_knitr>>=
SetImageSize(aspect_ratio= 2.0 * base_aspect_ratio)
@
<<functional_sens_plot_thresh0, cache=cache_knitr, fig.show='hold'>>=
source("Rscripts/appendix_functional_sens_results_thresh0.R", echo=knitr_debug, print.eval=TRUE)
@
\caption{The effect of prior perturbation on the expected number of distinct clusters. Left column: the original prior $p_0$ in red, the perturbed prior $p_c$, $\delta = 1$, in blue. Middle: linearly approximated vs.
re-fitted in-sample expected number of clusters. Right: linearly approximated vs. re-fitted predictive expected number of clusters.}
\end{figure}

% Because we have used a multiplicative perturbation, $f_\eta$
% is linear in $\epsilon$. We might expect this to improve the fidelity of a
% linear approximation. 
\begin{mdframed}[style=MyFrame]
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%
\vspace{-0.6in}
\section*{Conclusion}
\vspace{-0.3in}
%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{itemize}

\item The linear approximation provides a fast and reasonable alternative to re-evaluating the full model after changing the BNP prior. 

\item We applied the approximation to both parametric and functional perturbations to the stick-breaking prior.

\item We attempted to improve the linearity by approximating only the dependence of the global parameters on the prior parameter. We retained the non-linearity of the map from global parameters to posterior quantity. 

\end{itemize}
\end{mdframed}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%
{\bf References}
%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\renewcommand{\section}[2]{}%
\footnotesize{
  \bibliographystyle{abbrv}
  \bibliography{./references}
}
\end{minipage}\\

\begin{center}
\begin{minipage}[t]{\textwidth}
\small{
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% %%%%%%%%%%%%%%%%%%
% {\bf Acknowledgments}:
% % %%%%%%%%%%%%%%%%%%
% %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Ryan Giordano's research was funded in full by the
% Gordon and Betty Moore Foundation through Grant GBMF3834 and by the Alfred P. Sloan Foundation through Grant 2013-10-27 to the University of California, Berkeley. Runjing Liu's research was funded by the NSF graduate research fellowship. Tamara Broderick's research was supported in part by a Google Faculty Research Award and the Office of Naval Research under contract/grant number N00014-17-1-2072.\\
{\bf Contact: } rgiordano@berkeley.edu, runjing\_liu@berkeley.edu
}
\end{minipage}
\end{center}

\end{document}
