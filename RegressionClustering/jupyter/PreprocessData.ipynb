{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "import numpy as np\n",
    "import scipy as sp\n",
    "import inspect\n",
    "import os\n",
    "import sys\n",
    "import time\n",
    "\n",
    "import paragami\n",
    "\n",
    "from copy import deepcopy\n",
    "\n",
    "import bnpregcluster_runjingdev.regression_mixture_lib as gmm_lib\n",
    "\n",
    "import emfathy\n",
    "\n",
    "np.random.seed(42) # nothing special about this seed (we hope)!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(700, 9, 9) (700, 9)\n"
     ]
    }
   ],
   "source": [
    "orig_reg_params = dict()\n",
    "reg_params = dict()\n",
    "#datafile = 'fits/transformed_gene_regression_df7_degree3_genes10000.npz'\n",
    "datafile = 'fits/transformed_gene_regression_df7_degree3_genes1000.npz'\n",
    "with np.load(datafile) as infile:\n",
    "    reg_params['y_info'] = infile['y_info']\n",
    "    reg_params['beta_mean'] = infile['transformed_beta_mean']\n",
    "    reg_params['beta_info'] = infile['transformed_beta_info']\n",
    "    orig_reg_params['beta_mean'] = infile['beta_mean']\n",
    "    orig_reg_params['beta_info'] = infile['beta_info']\n",
    "    df = infile['df']\n",
    "    degree = infile['degree']\n",
    "\n",
    "    x_train = infile['x']\n",
    "    y_train = infile['y_train']\n",
    "\n",
    "num_genes = reg_params['beta_mean'].shape[0]\n",
    "obs_dim = reg_params['beta_mean'].shape[1]\n",
    "\n",
    "num_components = 100\n",
    "\n",
    "analysis_name = 'transformed_gene_regression_df{}_degree{}_genes{}_num_components{}_fit'.format(\n",
    "    df, degree, num_genes, num_components)\n",
    "print(reg_params['beta_info'].shape, reg_params['beta_mean'].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(42, 10) (700, 10) (700, 42) (700,)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "-2955.9225238187228"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Shrink the y_info with empirical bayes.\n",
    "\n",
    "e_tau = reg_params['y_info']\n",
    "e_log_tau = np.log(reg_params['y_info'])\n",
    "resid = y_train - np.einsum('ti,ni->nt', x_train, orig_reg_params['beta_mean'])\n",
    "print(x_train.shape, orig_reg_params['beta_mean'].shape, resid.shape, e_log_tau.shape)\n",
    "\n",
    "\n",
    "def get_hierarchical_gamma_log_prob(gamma, log_gamma, gamma_shape, gamma_rate):\n",
    "    # These can cause numerical difficulties, so separate them out\n",
    "    # for easier debugging.\n",
    "    t1 = gamma_shape * np.log(gamma_rate)\n",
    "    t2 = sp.special.gammaln(gamma_shape)\n",
    "    t3 = (gamma_shape - 1) * log_gamma\n",
    "    t4 = gamma_rate * gamma\n",
    "    return np.sum(t1 - t2 + t3 - t4)\n",
    "\n",
    "\n",
    "def get_regression_log_lik_by_nt(e_tau, e_log_tau, resid, prior_shape, prior_rate):\n",
    "    log_lik_by_nt = \\\n",
    "        -0.5 * e_tau[:, None] * (resid ** 2) + 0.5 * e_log_tau[:, None]\n",
    "    \n",
    "    return \\\n",
    "        np.sum(log_lik_by_nt) + \\\n",
    "        np.sum(get_hierarchical_gamma_log_prob(e_tau, e_log_tau, prior_shape, prior_rate))\n",
    "\n",
    "\n",
    "prior_shape = 3.0\n",
    "prior_rate = 4.0\n",
    "\n",
    "get_regression_log_lik_by_nt(e_tau, e_log_tau, resid, prior_shape, prior_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Misguided wishart stuff below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evs = np.full((num_genes, obs_dim), float('nan'))\n",
    "for n in range(num_genes):\n",
    "    evs[n, :] = np.linalg.eigvals(reg_params['beta_info'][n, :, :])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We could justifiably shrink the information matrices.  Let's do empirical bayes.\n",
    "plt.hist(evs[:, obs_dim - 1], 100);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "wishart_mean = np.mean(reg_params['beta_info'], axis=0)\n",
    "wishart_df = obs_dim + 2\n",
    "\n",
    "wishart_df_pattern = paragami.NumericScalarPattern(lb=obs_dim)\n",
    "\n",
    "def get_wishart_loss(wishart_df):\n",
    "    wishart_df = np.atleast_1d(wishart_df)[0]\n",
    "    wishart_lp = sp.stats.wishart.logpdf(\n",
    "        np.moveaxis(reg_params['beta_info'], 0, 2),\n",
    "        wishart_df,\n",
    "        scale=wishart_mean / wishart_df)\n",
    "    return -1 * np.sum(wishart_lp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wishart_opt = sp.optimize.minimize_scalar(\n",
    "    paragami.FlattenFunctionInput(\n",
    "        get_wishart_loss, patterns=wishart_df_pattern, free=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wishart_df_opt = wishart_df_pattern.fold(wishart_opt.x, free=True)[0]\n",
    "wishart_scale_opt = wishart_mean / wishart_df_opt\n",
    "print(wishart_df_opt)\n",
    "\n",
    "df_vec = np.linspace(obs_dim + 0.01, obs_dim + 30, 20)\n",
    "wish_lp = [ get_wishart_loss(df) for df in df_vec ] \n",
    "plt.plot(df_vec, wish_lp)\n",
    "plt.plot(wishart_df_opt, get_wishart_loss(wishart_df_opt), 'r+')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sanity check that we match the sufficient statistics\n",
    "\n",
    "wishart_draws = sp.stats.wishart.rvs(\n",
    "    df=wishart_df_opt, scale=wishart_scale_opt,\n",
    "    size=100000)\n",
    "print(np.linalg.norm(np.mean(wishart_draws, axis=0) -\n",
    "                     wishart_mean))\n",
    "\n",
    "mle_logdet = [ np.linalg.slogdet(wishart_draws[n, :, :])[1] for n in range(wishart_draws.shape[0]) ]\n",
    "obs_logdet = [ np.linalg.slogdet(reg_params['beta_info'][n, :, :])[1] for n in range(num_genes) ]\n",
    "\n",
    "print('logdet difference {} (se {})'.format(\n",
    "    (np.mean(mle_logdet) - np.mean(obs_logdet)),\n",
    "    np.std(mle_logdet) / np.sqrt(wishart_draws.shape[0])))\n",
    "\n",
    "wishart_draw_se = np.std(wishart_draws, axis=0) / np.sqrt(wishart_draws.shape[0])\n",
    "wishart_err = (np.mean(wishart_draws, axis=0) - wishart_mean) / wishart_draw_se\n",
    "plt.matshow(wishart_err); plt.colorbar()\n",
    "plt.matshow(wishart_draw_se); plt.colorbar()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above is not well-thought out.  What is the noise in the observations ``beta_info``?  This could get complicated.  Perhaps better to shrink ``y_info``."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plt.hist(reg_params['y_info'], 100);"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "bnpregcluster_runjingdev",
   "language": "python",
   "name": "bnpregcluster_runjingdev"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
