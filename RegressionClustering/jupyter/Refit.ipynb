{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "import numpy as np\n",
    "import inspect\n",
    "import os\n",
    "import sys\n",
    "import time\n",
    "\n",
    "import paragami\n",
    "import vittles\n",
    "\n",
    "from copy import deepcopy\n",
    "\n",
    "import bnpregcluster_runjingdev.regression_mixture_lib as gmm_lib\n",
    "\n",
    "np.random.seed(42) # nothing special about this seed (we hope)!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(328, 328)\n"
     ]
    }
   ],
   "source": [
    "#fitfile = './fits/transformed_gene_regression_df7_degree3_genes700_num_components100_fit.npz'\n",
    "#fitfile = './fits/transformed_gene_regression_df4_degree3_genes700_num_components30_fit.npz' # ok\n",
    "fitfile = 'fits/transformed_gene_regression_df7_degree3_genes700_num_components30_fit.npz'\n",
    "with np.load(fitfile) as infile:\n",
    "    gmm_params_pattern = paragami.get_pattern_from_json(\n",
    "        str(infile['gmm_params_pattern_json']))\n",
    "    opt_gmm_params = gmm_params_pattern.fold(\n",
    "        infile['opt_gmm_params_flat'], free=False)\n",
    "    prior_params_pattern = paragami.get_pattern_from_json(\n",
    "        str(infile['prior_params_pattern_json']))\n",
    "    prior_params = prior_params_pattern.fold(\n",
    "        infile['prior_params_flat'], free=False)\n",
    "    kl_hess = infile['kl_hess']\n",
    "    df = infile['df']\n",
    "    degree = infile['degree']\n",
    "    datafile = str(infile['datafile'])\n",
    "    num_components = int(infile['num_components'])\n",
    "\n",
    "reg_params = dict()\n",
    "with np.load(datafile) as infile:\n",
    "    reg_params['beta_mean'] = infile['transformed_beta_mean']\n",
    "    reg_params['beta_info'] = infile['transformed_beta_info']\n",
    "\n",
    "print(kl_hess.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setting preconditioner...\n",
      "Done.\n",
      "Optimizing...\n",
      "Iter 0: f = -114682.03556920\n",
      "Preconditioned iteration 1\n",
      "  Taking Newton step.\n",
      "Iter 1: f = -114682.03556920\n",
      "Iter 2: f = -114683.15041112\n",
      "1.114841926319059\n",
      "Iter 3: f = -114683.15041112\n",
      "Success.\n",
      "  Running preconditioned optimization.\n",
      "Iter 0: f = -114683.15041112\n",
      "Iter 1: f = -114683.18329204\n",
      "Iter 2: f = -114683.18510517\n",
      "Iter 3: f = -114683.18511243\n",
      "Iter 4: f = -114683.18511243\n",
      "Iter 4: f = -114683.18511243\n",
      "f_diff: 1.149543234583689\n",
      "Preconditioned iteration 2\n",
      "  Getting Hessian and preconditioner.\n"
     ]
    }
   ],
   "source": [
    "new_alpha = 10.0\n",
    "new_prior_params = deepcopy(prior_params)\n",
    "new_prior_params['probs_alpha'][:] = new_alpha\n",
    "\n",
    "gmm = gmm_lib.GMM(num_components, new_prior_params, reg_params)\n",
    "\n",
    "print('Setting preconditioner...')\n",
    "gmm.get_kl_conditioned.set_preconditioner_with_hessian(\n",
    "    hessian=kl_hess, ev_min=1e-6)\n",
    "print('Done.')\n",
    "\n",
    "init_x = gmm.gmm_params_pattern.flatten(opt_gmm_params, free=True)\n",
    "\n",
    "print('Optimizing...')\n",
    "gmm.conditioned_obj.reset()\n",
    "tic = time.time()\n",
    "gmm_opt, gmm_opt_x = gmm.optimize_fully(\n",
    "    init_x, verbose=True, kl_hess=kl_hess)\n",
    "opt_time = time.time() - tic\n",
    "print('Done.')\n",
    "print('Re-optimization time: {} seconds'.format(opt_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reopt_gmm_params = gmm.gmm_params_pattern.fold(gmm_opt_x, free=True)\n",
    "def comparison_plot(x, y):\n",
    "    plt.plot(x, x, 'r')\n",
    "    plt.plot(x, y, 'k.')\n",
    "    \n",
    "comparison_plot(opt_gmm_params['centroids'], reopt_gmm_params['centroids'])\n",
    "np.max(np.abs((opt_gmm_params['centroids'] - reopt_gmm_params['centroids'])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "e_z0 = gmm.get_e_z(opt_gmm_params)\n",
    "e_z1 = gmm.get_e_z(reopt_gmm_params)\n",
    "\n",
    "comparison_plot(np.mean(e_z0, axis=0), np.mean(e_z1, axis=0))\n",
    "np.max(np.abs(e_z0 - e_z1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "opt_gmm_params['stick_propn_mean'].shape\n",
    "num_components"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bnpmodeling_runjingdev.cluster_quantities_lib import get_e_num_large_clusters_from_ez\n",
    "from bnpmodeling_runjingdev.cluster_quantities_lib import get_e_number_clusters_from_logit_sticks\n",
    "\n",
    "n_samples = 10000\n",
    "threshold = 1\n",
    "predictive = True\n",
    "\n",
    "unif_samples = None\n",
    "unv_norm_samples = None\n",
    "if not predictive:\n",
    "    unif_samples = np.random.random((gmm.num_obs, n_samples))\n",
    "else:\n",
    "    unv_norm_samples = np.random.normal(\n",
    "        0, 1, size = (n_samples, num_components - 1))\n",
    "\n",
    "def get_posterior_quantity(gmm_params):\n",
    "    if not predictive:\n",
    "        e_z = gmm.get_e_z(gmm_params)\n",
    "        e_num, var_num = get_e_num_large_clusters_from_ez(\n",
    "            e_z,\n",
    "            threshold = threshold,\n",
    "            n_samples = None,\n",
    "            unif_samples = unif_samples)\n",
    "    else:\n",
    "        e_num = \\\n",
    "            get_e_number_clusters_from_logit_sticks(\n",
    "                gmm_params['stick_propn_mean'],\n",
    "                gmm_params['stick_propn_info'],\n",
    "                n_obs = gmm.num_obs,\n",
    "                threshold = threshold,\n",
    "                n_samples = None,\n",
    "                unv_norm_samples = unv_norm_samples)\n",
    "    return e_num\n",
    "\n",
    "get_posterior_quantity(opt_gmm_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "e_num0 = get_posterior_quantity(opt_gmm_params)\n",
    "e_num1 = get_posterior_quantity(reopt_gmm_params)\n",
    "print('Refit e:\\t{}\\nOrig e: \\t{}\\nDifference:\\t{}\\n'.format(\n",
    "    e_num1, e_num0, e_num1 - e_num0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prior_free = False\n",
    "\n",
    "get_kl_from_vb_free_prior_free = \\\n",
    "    paragami.FlattenFunctionInput(original_fun=\n",
    "        gmm.get_params_prior_kl,\n",
    "        patterns = [gmm.gmm_params_pattern, prior_params_pattern],\n",
    "        free = [True, prior_free],\n",
    "        argnums = [0, 1])\n",
    "\n",
    "gmm.get_params_prior_kl(opt_gmm_params, prior_params)\n",
    "\n",
    "get_kl_from_vb_free_prior_free(\n",
    "    gmm.gmm_params_pattern.flatten(opt_gmm_params, free=True),\n",
    "    prior_params_pattern.flatten(prior_params, free=prior_free))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "taylor_order = 1\n",
    "t0 = time.time()\n",
    "vb_sens = \\\n",
    "    vittles.ParametricSensitivityTaylorExpansion(\n",
    "        objective_function =    get_kl_from_vb_free_prior_free,\n",
    "        input_val0 =            gmm.gmm_params_pattern.flatten(opt_gmm_params, free=True),\n",
    "        hyper_val0 =            prior_params_pattern.flatten(prior_params, free=prior_free),\n",
    "        order =                 taylor_order,\n",
    "        hess0 =                 kl_hess)\n",
    "print('linear response Hessian time: {:.03f} secs'.format(time.time() - t0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict_gmm_params = \\\n",
    "    paragami.FoldFunctionInputAndOutput(\n",
    "        original_fun=vb_sens.evaluate_taylor_series,\n",
    "        input_patterns=prior_params_pattern,\n",
    "        input_free=prior_free,\n",
    "        input_argnums=[0],\n",
    "        output_patterns=gmm.gmm_params_pattern,\n",
    "        output_free=True,\n",
    "        output_retnums=[0])\n",
    "\n",
    "pred_gmm_params = predict_gmm_params(new_prior_params)\n",
    "\n",
    "e_num_pred = get_posterior_quantity(pred_gmm_params)\n",
    "\n",
    "print('Orig e: \\t{}\\nRefit e:\\t{}\\nPred e:\\t\\t{}\\nActual diff:\\t{:0.5}\\nPred diff:\\t{:0.5}'.format(\n",
    "    e_num0, e_num1, e_num_pred,\n",
    "    e_num1 - e_num0,\n",
    "    e_num_pred - e_num0))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "bnpgmm_runjingdev",
   "language": "python",
   "name": "bnpgmm_runjingdev"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
