{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/accounts/grad/runjing_liu/.conda/envs/bnp_sensitivity_jax/lib/python3.8/site-packages/jax/lib/xla_bridge.py:130: UserWarning: No GPU/TPU found, falling back to CPU.\n",
      "  warnings.warn('No GPU/TPU found, falling back to CPU.')\n"
     ]
    }
   ],
   "source": [
    "import jax\n",
    "\n",
    "import jax.numpy as np\n",
    "\n",
    "from numpy.polynomial.hermite import hermgauss\n",
    "\n",
    "from vb_lib import structure_model_lib, data_utils\n",
    "import vb_lib.structure_optimization_lib as s_optim_lib\n",
    "\n",
    "import paragami\n",
    "\n",
    "import time\n",
    "\n",
    "import pprint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as onp\n",
    "onp.random.seed(53453)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Simulate data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_obs = 1107 # number of individuals\n",
    "n_loci = 2810 # number of loci\n",
    "n_pop = 5 # number of true populations\n",
    "\n",
    "# array of size (n_obs x n_loci x 3)\n",
    "g_obs, true_pop_allele_freq, true_ind_admix_propn = \\\n",
    "    data_utils.draw_data(n_obs, n_loci, n_pop)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each individual is genotyped at `n_loci` loci. \n",
    "\n",
    "At each loci, the genotype is one of three classes: aa, Aa, or AA. \n",
    "\n",
    "For individual $n$ at loci $l$, `g_obs[n, l]` contains a one-hot-encoding of its genotype: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0. 0. 1.]\n"
     ]
    }
   ],
   "source": [
    "n = 0\n",
    "l = 0\n",
    "\n",
    "# either (1, 0, 0); (0, 1, 0); or (0, 0, 1)\n",
    "print(g_obs[n, l])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each individual is generated as a mixture of `n_pop` ancentral populations. \n",
    "\n",
    "The mixture proportions (as well as the number of ancenstral populations) need to be inferred."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([6.36557155e-01, 2.18859797e-01, 2.50443253e-02, 1.19538717e-01,\n",
       "       6.24815061e-09])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# the true mixture proportions for individual n\n",
    "# (we know this because we simulated the data): \n",
    "true_ind_admix_propn[n]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now at each loci $l$, there are two chromosomes, call them chromosome $1$ and chromosome $2$. \n",
    "\n",
    "Each chromosome belongs to a particular population. \n",
    "\n",
    "That is, the population indicators $z_{nl1}$ and $z_{nl2}$ are drawn from a categorical with mixture weights \n",
    "`true_ind_admix_propn[n]` for each $l$. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each population has a particular frequncy of the dominant allele $A$. This frequency also needs to inferred. \n",
    "\n",
    "Suppose loci $l$ of  individual $n$ comes from population $k = 0$. Then the probability that chromosome 1 is dominant (A) is: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9195577707266201"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# the probability of observing the dominant allele A \n",
    "# at locus l, if it comes from population k\n",
    "k = 0\n",
    "true_pop_allele_freq[l, k]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**In summary**, we need to infer: \n",
    "* A (`n_obs, n_pop`) array of individual mixture proportions. Rows sum to 1. \n",
    "* A (`n_loci, n_pop`) array of population allele frequncies. Entries are between 0 and 1. \n",
    "* A (`n_obs, n_loci, 2, n_pop`) array of cluster probabilities. Last dimension sums to 1, and give the probability of individual $n$, locus $l$, chromosome $i$ belonging to population $k$. \n",
    "\n",
    "Below, we will take truncated BNP approach, where we do not know `n_pop`. We will replace `n_pop` with a large positive integer, `k_approx`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get prior"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The individual mixture proportions are drawn from a Dirichlet stick-breaking process, with `dp_prior_alpha` the concentration parameter. \n",
    "\n",
    "The population allele frequencies at each $l$, $k$ are drawn iid from a Beta distribution, with parameters `allale_prior_alpha` and `allele_prior_beta`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'allele_prior_alpha': DeviceArray([1.], dtype=float64),\n",
      " 'allele_prior_beta': DeviceArray([1.], dtype=float64),\n",
      " 'dp_prior_alpha': DeviceArray([3.], dtype=float64)}\n"
     ]
    }
   ],
   "source": [
    "prior_params_dict, prior_params_paragami = \\\n",
    "    structure_model_lib.get_default_prior_params()\n",
    "\n",
    "pprint.pprint(prior_params_dict)\n",
    "\n",
    "prior_params_free = prior_params_paragami.flatten(prior_params_dict, free = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get VB params "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# number of components in truncated BNP approximation\n",
    "k_approx = 20"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The variational parameters: \n",
    "\n",
    "* `pop_freq_beta_params`: array of size (`n_loci`, `k_approx`, 2). Describes Beta distributed population frequncies. \n",
    "\n",
    "* `ind_admix_params`, which contains `stick_means` and `stick_infos`, each of size (`n_obs`, `k_approx - 1`). These are parameters for the logitnormal distribution on the stick-breaking proportions. (This differs from the usual approach of using a Beta distributed variational distribution on sticks. ). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OrderedDict:\n",
      "\t[pop_freq_beta_params] = NumericArrayPattern (2810, 20, 2) (lb=0.0, ub=inf)\n",
      "\t[ind_admix_params] = OrderedDict:\n",
      "\t[stick_means] = NumericArrayPattern (1107, 19) (lb=-inf, ub=inf)\n",
      "\t[stick_infos] = NumericArrayPattern (1107, 19) (lb=0.0, ub=inf)\n"
     ]
    }
   ],
   "source": [
    "# randomly initialized vb parameters along with corresponding pattern\n",
    "vb_params_dict, vb_params_paragami = \\\n",
    "    structure_model_lib.get_vb_params_paragami_object(n_obs,\n",
    "                                                      n_loci,\n",
    "                                                      k_approx,\n",
    "                                                      use_logitnormal_sticks = True)\n",
    "    \n",
    "print(vb_params_paragami)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The paragami object provides an easy way to convert parameter dictionaries to a flattened vector of real-valued, unconstrained parameters (`paragami.flatten`), or vice-versa (`paragami.fold`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(154466,)\n"
     ]
    }
   ],
   "source": [
    "# this is a real-valued vector of variational paramters\n",
    "vb_params = vb_params_paragami.flatten(vb_params_dict, free = True)\n",
    "print(vb_params.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since we use a logitnormal approximation to te stick-breaking proportions (instead of the usual approach of making the variational distributions also Beta distributed), we need to evaluate expectations under a logitnormal distribution. \n",
    "\n",
    "We do so using Gauss-Hermite quadrature. Compute locations and weights here ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "gh_deg = 8\n",
    "gh_loc, gh_weights = hermgauss(8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The KL objective"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Notice** that `vb_params_dict` does not include the `(n_obs, n_loci, 2, k_approx)` array of cluster belonging probabilities! An array of size `(n_obs, n_loci, 2, k_approx)` would be too large to instantiate in memory. \n",
    "\n",
    "Inside our kl objective, we implicitly set the optimal cluster belongings as a function of `vb_params` and never instantiate the full array of cluster belongings in memory. \n",
    "\n",
    "(We only ever instantiate a `(n_obs, 2, k_approx)` array by writing a for-loop through the loci). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let $\\theta$ be `vb_params`, and let $\\zeta$ be the array of *unconstrained* cluster belonging probabilities. \n",
    "Let $z$ be the *constrained* probabilities: in pseudo-code, `z = softmax(zeta, axis = -1)`.  \n",
    "\n",
    "Let $\\zeta^*(\\theta)$ be the optimal unconstrained cluster belonging probabilties. We need to work with unconstrained probabilities so that this optimality condition holds: \n",
    "\n",
    "\\begin{align}\n",
    "f_\\zeta(\\theta, \\zeta^*(\\theta)) = 0 \\quad \\forall\\; \\theta\n",
    "\\end{align}\n",
    "we will need this condition later. \n",
    "\n",
    "The function `get_kl` below is the function \n",
    "\\begin{align}\n",
    "    F(\\theta) := f(\\theta, \\zeta^*(\\theta))\n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_kl(vb_params): \n",
    "    \n",
    "    # below, the `detach_ez = False` argument \n",
    "    # allows gradients to backpropagate through the $z$'s. \n",
    "    # In other words, derivatives wrt to theta are\n",
    "    # the **total** derivative of the KL. \n",
    "    \n",
    "    # when `detach_ez = True`, derivatives will return \n",
    "    # the **partial** derivative of the KL. \n",
    "    \n",
    "    vb_params_dict = vb_params_paragami.fold(vb_params, free = True)\n",
    "\n",
    "    return structure_model_lib.get_kl(g_obs,\n",
    "                                      vb_params_dict, \n",
    "                                      prior_params_dict, \n",
    "                                      gh_loc,\n",
    "                                      gh_weights, \n",
    "                                      detach_ez = False)\n",
    "\n",
    "_get_kl_hvp = lambda v : jax.jvp(jax.grad(get_kl), (vb_params, ), (v, ))[1]\n",
    "get_kl_hvp = jax.jit(_get_kl_hvp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Computing hessian vector products is either super slow ... or just crashes the notebook entirely"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "v = jax.random.normal(key = jax.random.PRNGKey(0), \n",
    "                     shape = (len(vb_params), ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_kl_hvp = False\n",
    "if run_kl_hvp: \n",
    "    print('compiling ...')\n",
    "    t0 = time.time()\n",
    "    _ = get_kl_hvp(v).block_until_ready()\n",
    "    print('elapsed: {:.3f}sec'.format(time.time() - t0))\n",
    "    \n",
    "\n",
    "    t0 = time.time() \n",
    "    true_hvp = get_kl_hvp(v).block_until_ready()\n",
    "    print('Evaluation time: {:.3f}sec'.format(time.time() - t0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# My custom objective"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This class contains a custom implementation of the HVP using the Schur complement, which I detail below. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "compiling objective ... \n",
      "done. Elasped: 78.5147\n"
     ]
    }
   ],
   "source": [
    "stru_objective = s_optim_lib.StructureObjective(g_obs, \n",
    "                                                 vb_params_paragami,\n",
    "                                                 prior_params_dict, \n",
    "                                                 gh_loc, gh_weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the above optimality condition, we can derive the Schur complement decomposition of the Hessian: \n",
    "\\begin{align}\n",
    "    \\nabla^2 F(\\theta) = f_{\\theta\\theta} + f_{\\theta\\zeta}^Tf^{-1}_{\\zeta\\zeta}f_{\\theta\\zeta},\n",
    "\\end{align}\n",
    "where the RHS is evaluated at $(\\theta, \\zeta^*(\\theta))$. \n",
    "\n",
    "We can decompose the second term. Let $s$ be the mapping from $\\zeta$ to $z$ (this is just the softmax function). Then \n",
    "\\begin{align}\n",
    "f_{\\theta\\zeta}^Tf^{-1}_{\\zeta\\zeta}f_{\\theta\\zeta} = \n",
    "    [\\nabla_\\theta \\zeta^*]^T [\\nabla_\\zeta s]^T\n",
    "    f_{zz}\n",
    "    [\\nabla_\\zeta s]\n",
    "    [\\nabla_\\theta \\zeta^*]\n",
    "\\end{align}\n",
    "\n",
    "where $f_zz$ is the Hessian of the KL with respect to the clustering belonging probabilities.\n",
    "\n",
    "Note that\n",
    "* $f_{zz}$ is diagonal, with entries $\\{1 / z_{nlki}\\}$: all terms in the KL are linear in z except the entropy term \n",
    "(given by $z \\log z$). \n",
    "* $\\nabla_\\zeta s$ is the Jacobian of the softmax function, which has a closed form. \n",
    "* $\\nabla_\\theta \\zeta^*$ is sparse. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "elapsed: 14.257sec\n",
      "elapsed: 14.314sec\n",
      "elapsed: 14.222sec\n",
      "elapsed: 14.211sec\n",
      "elapsed: 14.283sec\n"
     ]
    }
   ],
   "source": [
    "# the above decomposition is implemented in the hvp method of `stru_objective`. \n",
    "\n",
    "for i in range(5): \n",
    "    t0 = time.time() \n",
    "    my_hvp = stru_objective.hvp(vb_params, v).block_until_ready()\n",
    "    print('elapsed: {:.3f}sec'.format(time.time() - t0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "if run_kl_hvp: \n",
    "    assert np.abs(my_hvp - true_hvp).max() < 1e-8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "bnp_sensitivity_jax",
   "language": "python",
   "name": "bnp_sensitivity_jax"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
