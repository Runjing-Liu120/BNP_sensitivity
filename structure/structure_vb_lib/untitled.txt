n_samples = 10

# define subsampled parameters 
sub_vb_params_dict, sub_vb_params_paragami = \
    structure_model_lib.get_vb_params_paragami_object(n_samples, n_loci, k_approx,
                                    use_logitnormal_sticks = use_logitnormal_sticks)

# define subsampled objective 
get_kl = lambda obs, params : structure_model_lib.get_kl(obs, params,
                                                               prior_params_dict,
                                                               gh_loc, gh_weights)
get_kl_free = paragami.FlattenFunctionInput(
                                original_fun=get_kl,
                                patterns = sub_vb_params_paragami,
                                free = True,
                                argnums = 1)

# jit the objective and gradient
get_kl_free_jitted = jax.jit(get_kl_free)
grad_kl = jax.jit(jax.grad(get_kl_free, argnums = 1))

# convert to numpy 
get_kl_np = lambda obs, params : onp.array(get_kl_free_jitted(obs, params))
grad_kl_np = lambda obs, params : onp.array(grad_kl(obs, params))

for i in range(10): 
    
    # subsample
    samples = onp.random.choice(g_obs.shape[0], n_samples)

    g_obs_subsampled = g_obs[samples]

    sub_vb_params_dict['ind_mix_stick_propn_mean'] = \
        vb_params_dict['ind_mix_stick_propn_mean'][samples]
    sub_vb_params_dict['ind_mix_stick_propn_info'] = \
        vb_params_dict['ind_mix_stick_propn_info'][samples]

    params = sub_vb_params_paragami.flatten(sub_vb_params_dict, free = True)
    
    if i == 0: 
        print('compiling ...')
        _ = get_kl_free_jitted(g_obs_subsampled, params)
        _ = grad_kl(g_obs_subsampled, params)

    print(i)
    out = optimize.minimize(lambda x : get_kl_np(g_obs_subsampled, x),
                        x0 = onp.array(params),
                        jac = lambda x : grad_kl_np(g_obs_subsampled, x),
                        method='L-BFGS-B', 
                        options = {'maxiter':1})
                        
                        
# def get_e_loglik(g_obs,
#                     e_log_pop_freq, e_log_1m_pop_freq, \
#                     e_log_sticks, e_log_1m_sticks,
#                     detach_ez): 

#     n_obs = g_obs.shape[0]
#     n_loci = g_obs.shape[1]
    
#     e_log_cluster_probs = \
#         modeling_lib.get_e_log_cluster_probabilities_from_e_log_stick(
#                             e_log_sticks, e_log_1m_sticks)
#     def body_fun(val, i): 
#         n = i % n_obs 
#         l = i // n_obs
#         return get_e_loglik_nl(g_obs[n, l], e_log_pop_freq[l], e_log_1m_pop_freq[l],
#                         e_log_cluster_probs[n], detach_ez) + val

#     scan_fun = lambda val, x : (body_fun(val, x), None)
    
#     init_val = np.array([0., 0.])
#     out = jax.lax.scan(scan_fun, init_val,
#                         xs = np.arange(n_obs * n_loci))[0]

#     e_loglik = out[0]
#     z_entropy = out[1]
    
#     return e_loglik, z_entropy 
