{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook demonstrates the use of our linear approximation for sensitivity with respect to the Dirichlet process $\\alpha$ parameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "No module named 'BNP_modeling'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-e2dcd45afe83>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;31m# sys.path.insert(0, '../../BNP_modeling/')\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mbnpgmm_runjingdev\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgmm_clustering_lib\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mgmm_lib\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mbnpgmm_runjingdev\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodeling_lib\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mbnpgmm_runjingdev\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils_lib\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/git_repos/BNP_sensitivity/GMM_clustering/bnpgmm_runjingdev/gmm_clustering_lib.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0msys\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0msys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minsert\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'../../BNP_modeling/'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mBNP_modeling\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcluster_quantities_lib\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mcluster_lib\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mBNP_modeling\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodeling_lib\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mmodeling_lib\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mImportError\u001b[0m: No module named 'BNP_modeling'"
     ]
    }
   ],
   "source": [
    "from autograd import numpy as np\n",
    "from autograd import scipy as sp\n",
    "\n",
    "from numpy.polynomial.hermite import hermgauss\n",
    "\n",
    "np.random.seed(453453)\n",
    "\n",
    "import paragami\n",
    "\n",
    "# BNP sensitivity libraries\n",
    "import sys\n",
    "# sys.path.insert(0, '../libraries/')\n",
    "# sys.path.insert(0, '../../BNP_modeling/')\n",
    "\n",
    "import bnpgmm_runjingdev.gmm_clustering_lib as gmm_lib\n",
    "import bnpgmm_runjingdev.modeling_lib \n",
    "import bnpgmm_runjingdev.utils_lib\n",
    "import bnpgmm_runjingdev.cluster_quantities_lib as cluster_lib\n",
    "import bnpgmm_runjingdev.optimization_lib\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import matplotlib.cm as cm\n",
    "\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "from copy import deepcopy\n",
    "\n",
    "import vittles\n",
    "\n",
    "import json "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We load and plot the iris dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# load iris data\n",
    "dataset_name = 'iris'\n",
    "features, iris_species = utils_lib.load_data()\n",
    "dim = features.shape[1]\n",
    "n_obs = len(iris_species)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# run PCA\n",
    "pca_fit = PCA()\n",
    "pca_fit.fit(features)\n",
    "pc_features = pca_fit.transform(features)\n",
    "\n",
    "# plot\n",
    "cmap = cm.get_cmap(name='gist_rainbow')\n",
    "colors1 = [cmap(k * 50) for k in range(30)]\n",
    "\n",
    "fig1 = plt.figure(1)\n",
    "fig = fig1.add_subplot(111)\n",
    "utils_lib.plot_clusters(pc_features[:, 0], pc_features[:, 1], iris_species, colors1, fig)\n",
    "fig.set_xlabel('PC1')\n",
    "fig.set_ylabel('PC2')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Set up model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prior parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Get priors\n",
    "prior_params_dict, prior_params_paragami = gmm_lib.get_default_prior_params(dim)\n",
    "prior_params_dict['alpha'] = 3.5\n",
    "print(prior_params_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Variational parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# number of clusters in variational distribution \n",
    "k_approx = 30"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Gauss-Hermite points for integrating logitnormal stick-breaking prior\n",
    "gh_deg = 8\n",
    "gh_loc, gh_weights = hermgauss(gh_deg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# get vb parameters\n",
    "vb_params_dict, vb_params_paragami = gmm_lib.get_vb_params_paragami_object(dim, k_approx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "stick_propn_mean = vb_params_dict['stick_propn_mean']\n",
    "stick_propn_info = vb_params_dict['stick_propn_info']"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "from modeling_lib import ef"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "stick_propn_mean = stick_propn_mean[:, None]\n",
    "stick_propn_info = stick_propn_info[:, None]"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "e_log_v, e_log_1mv = \\\n",
    "        ef.get_e_log_logitnormal(\n",
    "            lognorm_means = stick_propn_mean,\n",
    "            lognorm_infos = stick_propn_info,\n",
    "            gh_loc = gh_loc,\n",
    "            gh_weights = gh_weights)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "e_log_v.shape"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "zeros_shape = stick_propn_mean.shape[0:-1] + (1,)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "e_log_stick_remain = np.concatenate([np.zeros(zeros_shape), \\\n",
    "                                        np.cumsum(e_log_1mv, axis = -1)], axis = -1)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "np.cumsum(e_log_1mv, axis = -1)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "e_log_stick_remain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cluster_lib.get_e_number_clusters_from_logit_sticks(stick_propn_mean, stick_propn_info, n_obs, n_samples=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# the objective \n",
    "gmm_lib.get_kl(features, vb_params_dict, prior_params_dict, gh_loc, gh_weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Optimize the KL objective"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialize with k-means"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# run a kmeans init\n",
    "n_kmeans_init = 50\n",
    "init_vb_free_params, init_vb_params_dict, init_ez = \\\n",
    "    utils_lib.cluster_and_get_k_means_inits(features, vb_params_paragami, \n",
    "                                                n_kmeans_init = n_kmeans_init, \n",
    "                                                  seed = 453453)\n",
    "\n",
    "print('initial kl: ')\n",
    "print(gmm_lib.get_kl(features, init_vb_params_dict, prior_params_dict, gh_loc, gh_weights))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set up KL objective as a function of unconstrained VB parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# get loss as a function of vb parameters\n",
    "get_vb_params_loss = paragami.FlattenFunctionInput(\n",
    "                                original_fun=gmm_lib.get_kl, \n",
    "                                patterns = vb_params_paragami,\n",
    "                                free = True,\n",
    "                                argnums = 1)\n",
    "\n",
    "get_loss = lambda x : get_vb_params_loss(features, x, prior_params_dict, gh_loc, gh_weights)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run Newton "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "vb_opt = optimization_lib.optimize_full(get_loss, init_vb_free_params,\n",
    "                    bfgs_max_iter = 500, netwon_max_iter = 50,\n",
    "                    max_precondition_iter = 10,\n",
    "                    gtol=1e-8, ftol=1e-8, xtol=1e-8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "vb_opt_dict = vb_params_paragami.fold(vb_opt, free = True)\n",
    "e_z_opt = gmm_lib.get_optimal_z_from_vb_params_dict(features, vb_opt_dict, gh_loc, gh_weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def plot_results_from_vb_params_dict(pca_fit, vb_params_dict, e_z, fig): \n",
    "    # we plot in PCA space\n",
    "    bnp_centroids_pc, bnp_cluster_covs_pc = \\\n",
    "        utils_lib.transform_params_to_pc_space(pca_fit, vb_params_dict['centroids'], \n",
    "                                               np.linalg.inv(vb_params_dict['gamma']))\n",
    "    \n",
    "    bnp_clusters = np.argmax(e_z, axis = 1)\n",
    "    \n",
    "    cmap = cm.get_cmap(name='gist_rainbow')\n",
    "    colors1 = [cmap(k * 50) for k in range(30)]\n",
    "\n",
    "    utils_lib.plot_clusters(pc_features[:, 0], pc_features[:, 1], bnp_clusters, \\\n",
    "                  colors1, fig, \\\n",
    "                  centroids = bnp_centroids_pc[:, np.unique(bnp_clusters)], \n",
    "                cov = bnp_cluster_covs_pc[np.unique(bnp_clusters)])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Newton results\n",
    "fig1 = plt.figure(1)\n",
    "fig = fig1.add_subplot(111)\n",
    "\n",
    "plot_results_from_vb_params_dict(pca_fit, vb_opt_dict, e_z_opt, fig)\n",
    "fig.set_xlabel('PC1', fontsize = 18)\n",
    "fig.set_ylabel('PC2', fontsize = 18)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parametric sensitivity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get the hyper parameter sensitivity object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "get_prior_vb_loss = paragami.FlattenFunctionInput(original_fun=gmm_lib.get_kl, \n",
    "                                    patterns = [vb_params_paragami, prior_params_paragami],\n",
    "                                    free = True,\n",
    "                                    argnums = [1, 2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "vb_sens = \\\n",
    "    vittles.HyperparameterSensitivityLinearApproximation(\n",
    "        objective_fun = lambda x, y: get_prior_vb_loss(features, x, y, gh_loc, gh_weights),\n",
    "        opt_par_value = vb_opt, \n",
    "        hyper_par_value = prior_params_paragami.flatten(prior_params_dict, free=True),\n",
    "        validate_optimum=False,\n",
    "        hessian_at_opt=None,\n",
    "        cross_hess_at_opt=None,\n",
    "        factorize_hessian=True,\n",
    "        hyper_par_objective_fun=None,\n",
    "        grad_tol=1e-8)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set new prior"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "new_alpha = np.array([5.])\n",
    "\n",
    "prior_params_pert_dict = deepcopy(prior_params_dict)\n",
    "prior_params_pert_dict['alpha'] = new_alpha\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print('initial alpha, ', prior_params_dict['alpha'])\n",
    "print('perturbed alpha, ', prior_params_pert_dict['alpha'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### get linear response prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "lr_vb_free_params = \\\n",
    "    vb_sens.predict_opt_par_from_hyper_par(\n",
    "        prior_params_paragami.flatten(prior_params_pert_dict, free = True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print('l_inf diff: ', np.max(np.abs(lr_vb_free_params - vb_opt)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### re-optimize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# set up new objective, with the prior changed\n",
    "get_pert_loss = lambda x : get_vb_params_loss(features, x, prior_params_pert_dict, gh_loc, gh_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Run newton\n",
    "vb_opt_pert = optimization_lib.optimize_full(get_pert_loss, deepcopy(vb_opt),\n",
    "                    bfgs_max_iter = 500, netwon_max_iter = 50,\n",
    "                    max_precondition_iter = 10,\n",
    "                    gtol=1e-8, ftol=1e-8, xtol=1e-8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compare linear response with refitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def print_diff_plot(lr_vb_free_params, vb_opt_pert, vb_opt): \n",
    "    plt.plot(lr_vb_free_params - vb_opt, \n",
    "             vb_opt_pert - vb_opt, \n",
    "             '+', color = 'red')\n",
    "\n",
    "    plt.plot(lr_vb_free_params - vb_opt, \n",
    "            lr_vb_free_params - vb_opt, \n",
    "             '-', color = 'blue')\n",
    "\n",
    "    plt.xlabel('lr')\n",
    "    plt.ylabel('re-optimized')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print_diff_plot(lr_vb_free_params, vb_opt_pert, vb_opt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Fit for a range of alpha "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "alpha_list = np.arange(1, 15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def refit_with_alpha(alpha, vb_opt):\n",
    "    # sets new alpha, returns new vb optimum\n",
    "    \n",
    "    # set new prior\n",
    "    new_alpha = np.array([alpha])\n",
    "\n",
    "    prior_params_pert_dict = deepcopy(prior_params_dict)\n",
    "    prior_params_pert_dict['alpha'] = new_alpha\n",
    "    \n",
    "    # set up new objective, with the prior changed\n",
    "    get_pert_loss = lambda x : get_vb_params_loss(features, x, prior_params_pert_dict, gh_loc, gh_weights)\n",
    "    \n",
    "    # optimize\n",
    "    vb_opt_pert = optimization_lib.optimize_full(get_pert_loss,deepcopy(vb_opt),\n",
    "                    bfgs_max_iter = 500, netwon_max_iter = 50,\n",
    "                    max_precondition_iter = 10,\n",
    "                    gtol=1e-8, ftol=1e-8, xtol=1e-8)\n",
    "    \n",
    "    return vb_opt_pert"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Re-optimize for range of alphas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print('alphas: ', alpha_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "vb_pert_list = []\n",
    "for alpha in alpha_list: \n",
    "    print('re-optimzing with alpha = ', alpha)\n",
    "    \n",
    "    vb_pert_list.append(refit_with_alpha(alpha, vb_opt))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### get linear response prediction for each alpha"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "lr_list = []\n",
    "\n",
    "for alpha in alpha_list: \n",
    "    \n",
    "    # set alpha \n",
    "    new_alpha = np.array([alpha])\n",
    "    prior_params_pert_dict = deepcopy(prior_params_dict)\n",
    "    prior_params_pert_dict['alpha'] = new_alpha\n",
    "    \n",
    "    # get linear response\n",
    "    lr_list.append(vb_sens.predict_opt_par_from_hyper_par(\n",
    "        prior_params_paragami.flatten(prior_params_pert_dict, free = True)))\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### examine results "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for i in range(len(alpha_list)): \n",
    "    plt.figure()\n",
    "    print_diff_plot(lr_list[i], vb_pert_list[i], vb_opt)\n",
    "    \n",
    "    plt.title('alpha = {}'.format(alpha_list[i]))\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Check results on number of clusters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### results on number of in-sample clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "threshold = 0\n",
    "\n",
    "lr_e_num_clusters_vec = np.zeros(len(alpha_list))\n",
    "refit_e_num_clusters_vec = np.zeros(len(alpha_list))\n",
    "\n",
    "for i in range(len(alpha_list)): \n",
    "    # number of clusters as predicted by linear response\n",
    "    lr_e_num_clusters, _ = gmm_lib.get_e_num_clusters_from_free_par(features, vb_params_paragami, \n",
    "                                    lr_list[i],\n",
    "                                    gh_loc, gh_weights,\n",
    "                                    threshold = threshold,\n",
    "                                    n_samples = 100000)\n",
    "    \n",
    "    lr_e_num_clusters_vec[i] = lr_e_num_clusters\n",
    "    \n",
    "    # number of clusters after refitting \n",
    "    refit_e_num_clusters, _ = gmm_lib.get_e_num_clusters_from_free_par(features, vb_params_paragami, \n",
    "                                    vb_pert_list[i],\n",
    "                                    gh_loc, gh_weights,\n",
    "                                    threshold = threshold,\n",
    "                                    n_samples = 100000)\n",
    "    \n",
    "    refit_e_num_clusters_vec[i] = refit_e_num_clusters\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "plt.plot(alpha_list, lr_e_num_clusters_vec, '+--')\n",
    "plt.plot(alpha_list, refit_e_num_clusters_vec, '+-')\n",
    "\n",
    "plt.xlabel('alpha')\n",
    "plt.ylabel('num posterior clusters')\n",
    "plt.legend(('lr', 'refit'))\n",
    "\n",
    "plt.axvline(prior_params_dict['alpha'], color = 'red', linestyle = 'dashed')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Results on posterior predictive clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "threshold = 0\n",
    "\n",
    "lr_e_num_pred_clusters_vec = np.zeros(len(alpha_list))\n",
    "refit_e_num_pred_clusters_vec = np.zeros(len(alpha_list))\n",
    "\n",
    "for i in range(len(alpha_list)): \n",
    "     # number of clusters as predicted by linear response\n",
    "    lr_e_num_pred_clusters = gmm_lib.get_e_num_pred_clusters_from_vb_free_params(vb_params_paragami, \n",
    "                                    lr_list[i],\n",
    "                                    n_obs = np.shape(features)[0], \n",
    "                                    threshold = threshold,\n",
    "                                    n_samples = 100000)\n",
    "    \n",
    "    lr_e_num_pred_clusters_vec[i] = lr_e_num_pred_clusters\n",
    "    \n",
    "    # number of clusters after refitting \n",
    "    refit_e_num_pred_clusters = gmm_lib.get_e_num_pred_clusters_from_vb_free_params(vb_params_paragami, \n",
    "                                    vb_pert_list[i],\n",
    "                                    n_obs = np.shape(features)[0], \n",
    "                                    threshold = threshold,\n",
    "                                    n_samples = 100000)\n",
    "    \n",
    "    refit_e_num_pred_clusters_vec[i] = refit_e_num_pred_clusters\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "plt.plot(alpha_list, lr_e_num_pred_clusters_vec, '+--')\n",
    "plt.plot(alpha_list, refit_e_num_pred_clusters_vec, '+-')\n",
    "\n",
    "plt.xlabel('alpha')\n",
    "plt.ylabel('num posterior clusters')\n",
    "plt.legend(('lr', 'refit'))\n",
    "\n",
    "plt.axvline(prior_params_dict['alpha'], color = 'red', linestyle = 'dashed')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "bnpgmm_runjingdev",
   "language": "python",
   "name": "bnpgmm_runjingdev"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
