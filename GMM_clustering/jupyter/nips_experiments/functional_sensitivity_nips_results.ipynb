{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook reproduces the functional sensitivity results in the paper\n",
    "\n",
    "\"Evaluating Sensitivity to the Stick Breaking Prior in Bayesian Nonparametrics\"\n",
    "https://arxiv.org/abs/1810.06587\n",
    "\n",
    "\n",
    "It loads the model fits from './iris_fits/', so make sure to run the notebook `fit_base_model.ipynb` first. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/rgiordan/Documents/git_repos/BNP_sensitivity/GMM_clustering/venv/lib/python3.5/site-packages/autograd/core.py:290: UserWarning: \n",
      "The defvjp method is deprecated. See the update guide and tutorial:\n",
      "https://github.com/HIPS/autograd/blob/master/docs/updateguide.md\n",
      "https://github.com/HIPS/autograd/blob/master/docs/tutorial.md\n",
      "  warnings.warn(deprecation_msg)\n"
     ]
    },
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'cluster_quantities_lib'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-f06344fa1bd2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mbnpmodeling_runjingdev\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mmodeling_lib\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mbnpgmm_runjingdev\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mutils_lib\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mbnpgmm_runjingdev\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mcluster_quantities_lib\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mcluster_lib\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mbnpmodeling_runjingdev\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0moptimization_lib\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mbnpgmm_runjingdev\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mfunctional_sensitivity_lib\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mfun_sens_lib\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mImportError\u001b[0m: cannot import name 'cluster_quantities_lib'"
     ]
    }
   ],
   "source": [
    "import autograd\n",
    "from autograd import numpy as np\n",
    "from autograd import scipy as sp\n",
    "\n",
    "from numpy.polynomial.hermite import hermgauss\n",
    "\n",
    "np.random.seed(453453)\n",
    "\n",
    "import paragami\n",
    "\n",
    "# BNP sensitivity libraries\n",
    "# import sys\n",
    "# sys.path.insert(0, '../../libraries/')\n",
    "from bnpgmm_runjingdev import gmm_clustering_lib as gmm_lib\n",
    "from bnpmodeling_runjingdev import modeling_lib \n",
    "from bnpgmm_runjingdev import utils_lib\n",
    "from bnpgmm_runjingdev import cluster_quantities_lib as cluster_lib\n",
    "from bnpmodeling_runjingdev import optimization_lib\n",
    "from bnpgmm_runjingdev import functional_sensitivity_lib as fun_sens_lib \n",
    "\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import matplotlib.cm as cm\n",
    "\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "from copy import deepcopy\n",
    "\n",
    "import json \n",
    "\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# load iris data\n",
    "dataset_name = 'iris'\n",
    "features, iris_species = utils_lib.load_data()\n",
    "dim = features.shape[1]\n",
    "n_obs = len(iris_species)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run PCA\n",
    "pca_fit = PCA()\n",
    "pca_fit.fit(features)\n",
    "pc_features = pca_fit.transform(features)\n",
    "\n",
    "# plot\n",
    "cmap = cm.get_cmap(name='gist_rainbow')\n",
    "colors1 = [cmap(k * 50) for k in range(30)]\n",
    "\n",
    "fig1 = plt.figure(1)\n",
    "fig = fig1.add_subplot(111)\n",
    "utils_lib.plot_clusters(pc_features[:, 0], pc_features[:, 1], iris_species, colors1, fig)\n",
    "fig.set_xlabel('PC1')\n",
    "fig.set_ylabel('PC2')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load results from previous fit."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It loads the model fits from './iris_fits/', so make sure to run the notebook `fit_base_model.ipynb` first. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def assert_optimizer(features, vb_opt_dict, vb_params_paragami, prior_params_dict, gh_loc, gh_weights): \n",
    "    # this function checks that vb_opt_dict are at a kl optimum for the given \n",
    "    # prior parameters\n",
    "    \n",
    "    get_vb_params_loss = paragami.Functor(original_fun=gmm_lib.get_kl, argnums=1)\n",
    "    get_vb_params_loss.cache_args(features, None, prior_params_dict,\n",
    "                                    gh_loc, gh_weights)\n",
    "    get_loss = paragami.FlattenedFunction(original_fun=get_vb_params_loss,\n",
    "                                            patterns=vb_params_paragami,\n",
    "                                            free=True)\n",
    "    grad_get_loss = autograd.grad(get_loss)\n",
    "    linf_grad = np.max(np.abs(grad_get_loss(vb_params_paragami.flatten(vb_opt_dict, free = True))))\n",
    "    \n",
    "    assert  linf_grad < 1e-5, 'error: {}'.format(linf_grad)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "init_alpha = 8.0\n",
    "\n",
    "# load optimal vb parameters at init_alpha\n",
    "vb_opt_dict, vb_params_paragami, data =\\\n",
    "    paragami.load_folded('./iris_fits/iris_fits_alpha{}.npz'.format(init_alpha))\n",
    "\n",
    "# free optimal vb_parameters\n",
    "vb_opt = vb_params_paragami.flatten(vb_opt_dict, free = True)\n",
    "\n",
    "# load prior parameters\n",
    "prior_params_dict, prior_params_paragami = \\\n",
    "    gmm_lib.get_default_prior_params(dim = vb_opt_dict['centroids'].shape[0])\n",
    "prior_params_dict['alpha'] = data['alpha']\n",
    "\n",
    "# other parameters\n",
    "gh_deg = int(data['gh_deg'])\n",
    "gh_loc, gh_weights = hermgauss(gh_deg)\n",
    "\n",
    "# optimal e_z\n",
    "e_z_opt = gmm_lib.get_optimal_z_from_vb_params_dict(features, vb_opt_dict, gh_loc, gh_weights)\n",
    "\n",
    "# assert we are at an optimum \n",
    "assert_optimizer(features, vb_opt_dict, vb_params_paragami, prior_params_dict, gh_loc, gh_weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def plot_results_from_vb_params_dict(pca_fit, vb_params_dict, e_z, fig): \n",
    "    # we plot in PCA space\n",
    "    bnp_centroids_pc, bnp_cluster_covs_pc = \\\n",
    "        utils_lib.transform_params_to_pc_space(pca_fit, vb_params_dict['centroids'], \n",
    "                                               np.linalg.inv(vb_params_dict['gamma']))\n",
    "    \n",
    "    bnp_clusters = np.argmax(e_z, axis = 1)\n",
    "    \n",
    "    cmap = cm.get_cmap(name='gist_rainbow')\n",
    "    colors1 = [cmap(k * 50) for k in range(30)]\n",
    "\n",
    "    utils_lib.plot_clusters(pc_features[:, 0], pc_features[:, 1], bnp_clusters, \\\n",
    "                  colors1, fig, \\\n",
    "                  centroids = bnp_centroids_pc[:, np.unique(bnp_clusters)], \n",
    "                cov = bnp_cluster_covs_pc[np.unique(bnp_clusters)])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Newton results\n",
    "fig1 = plt.figure(1)\n",
    "fig = fig1.add_subplot(111)\n",
    "\n",
    "plot_results_from_vb_params_dict(pca_fit, vb_opt_dict, e_z_opt, fig)\n",
    "fig.set_xlabel('PC1', fontsize = 18)\n",
    "fig.set_ylabel('PC2', fontsize = 18)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define a prior perturbation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this is the phi chosen in figure 2 of the paper\n",
    "\n",
    "def log_phi(logit_v):\n",
    "    return(sp.special.expit(logit_v))\n",
    "\n",
    "def phi(logit_v):\n",
    "    return np.exp(log_phi(logit_v))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logit_v_grid = np.linspace(-8, 8, 200)\n",
    "\n",
    "v_grid = np.exp(logit_v_grid) / (1 + np.exp(logit_v_grid))\n",
    "\n",
    "print('phi max: ', np.max(phi(logit_v_grid)))\n",
    "log_phi_max = np.max(np.abs(np.log(phi(logit_v_grid))))\n",
    "print('log phi max: ', log_phi_max)\n",
    "\n",
    "def rescaled_log_phi(logit_v):\n",
    "    return 10 * log_phi(logit_v) / log_phi_max\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prior_perturbation = fun_sens_lib.PriorPerturbation(vb_params_dict = vb_opt_dict, \n",
    "                                                    alpha0 = prior_params_dict['alpha'],\n",
    "                                                    gh_loc = gh_loc, \n",
    "                                                    gh_weights = gh_weights, \n",
    "                                                    log_phi = rescaled_log_phi, logit_v_ub=8, logit_v_lb = -8)\n",
    "print(\n",
    "    prior_perturbation.log_norm_p0,\n",
    "    prior_perturbation.log_norm_pc,\n",
    "    prior_perturbation.log_norm_p0_logit,\n",
    "    prior_perturbation.log_norm_pc_logit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prior_perturbation.set_epsilon(1.0)\n",
    "\n",
    "plt.figure(1, figsize=(18, 5))\n",
    "\n",
    "plt.subplot(141)\n",
    "plt.plot(logit_v_grid, prior_perturbation.get_log_p0_logit(logit_v_grid))\n",
    "plt.plot(logit_v_grid, prior_perturbation.get_log_pc_logit(logit_v_grid))\n",
    "plt.title('Log priors in logit space')\n",
    "\n",
    "plt.subplot(142)\n",
    "plt.plot(logit_v_grid, prior_perturbation.log_phi(logit_v_grid))\n",
    "plt.title('log phi in logit space')\n",
    "\n",
    "plt.subplot(143)\n",
    "plt.plot(v_grid, np.exp(prior_perturbation.get_log_p0(v_grid)))\n",
    "plt.plot(v_grid, np.exp(prior_perturbation.get_log_pc(v_grid)))\n",
    "plt.title('Priors in stick space')\n",
    "\n",
    "plt.subplot(144)\n",
    "plt.plot(logit_v_grid, np.exp(prior_perturbation.get_log_p0_logit(logit_v_grid)))\n",
    "plt.plot(logit_v_grid, np.exp(prior_perturbation.get_log_pc_logit(logit_v_grid)))\n",
    "plt.title('Priors in logit space')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Increase number of Gauss-hermite points."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# First fit with no perturbation but the increased number of gh points.\n",
    "\n",
    "gh_loc, gh_weights = hermgauss(20)\n",
    "\n",
    "# get loss as a function of vb parameters\n",
    "get_vb_params_loss = paragami.Functor(original_fun=gmm_lib.get_kl, argnums=1)\n",
    "get_vb_params_loss.cache_args(features, None, prior_params_dict,\n",
    "                                gh_loc, gh_weights)\n",
    "\n",
    "# Get loss as a function vb_free_params\n",
    "get_loss = paragami.FlattenedFunction(original_fun=get_vb_params_loss,\n",
    "                                        patterns=vb_params_paragami,\n",
    "                                        free=True)\n",
    "\n",
    "init_vb_free_params = deepcopy(vb_opt)\n",
    "vb_opt, _ = optimization_lib.precondition_and_optimize(get_loss, init_vb_free_params,\n",
    "                    maxiter = 50, gtol=1e-8)\n",
    "\n",
    "print('Done. ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "vb_opt_dict = vb_params_paragami.fold(vb_opt, free = True)\n",
    "e_z_opt = gmm_lib.get_optimal_z_from_vb_params_dict(features, vb_opt_dict, gh_loc, gh_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot results\n",
    "fig1 = plt.figure(1)\n",
    "fig = fig1.add_subplot(111)\n",
    "\n",
    "plot_results_from_vb_params_dict(pca_fit, vb_opt_dict, e_z_opt, fig)\n",
    "fig.set_xlabel('PC1', fontsize = 18)\n",
    "fig.set_ylabel('PC2', fontsize = 18)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Define linear approximation class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "epsilon_param_dict = {'epsilon': np.array([1.0])}\n",
    "\n",
    "epsilon_param_paragami = paragami.PatternDict() \n",
    "epsilon_param_paragami['epsilon'] = \\\n",
    "        paragami.NumericArrayPattern(shape=(1, ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# we will use this for the sensitivity class\n",
    "get_epsilon_vb_loss = paragami.Functor(\n",
    "    original_fun=fun_sens_lib.get_perturbed_kl, argnums=[1, 2])\n",
    "\n",
    "get_epsilon_vb_loss.cache_args(features, None, None, rescaled_log_phi, \n",
    "                     prior_params_dict, gh_loc, gh_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the only part of the kl that depends on epsilon\n",
    "hyper_par_objective_fun = \\\n",
    "    paragami.Functor(fun_sens_lib.get_e_log_perturbation, argnums = [1, 2])\n",
    "\n",
    "# the kl as a function of both the vb parameters and the hyperparameter\n",
    "hyper_par_objective_fun.cache_args(rescaled_log_phi, None, None, \n",
    "                           gh_loc, gh_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute hyper sensitivity object\n",
    "epsilon_sens = \\\n",
    "    paragami.HyperparameterSensitivityLinearApproximation(\n",
    "        objective_fun=           get_epsilon_vb_loss,\n",
    "        opt_par_pattern=         vb_params_paragami,\n",
    "        hyper_par_pattern=       epsilon_param_paragami,\n",
    "        opt_par_folded_value=    vb_params_paragami.fold(vb_opt, free = True),\n",
    "        hyper_par_folded_value=  epsilon_param_paragami.fold(0.0, free = False),\n",
    "        opt_par_is_free=         True,\n",
    "        hyper_par_is_free=       False,\n",
    "        hyper_par_objective_fun = hyper_par_objective_fun,\n",
    "        grad_tol=                1e-5)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fit with perturbation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epsilon = 1.0 \n",
    "print('Epsilon: ', epsilon)\n",
    "\n",
    "vb_pert_pred = \\\n",
    "    epsilon_sens.predict_opt_par_from_hyper_par(epsilon_param_dict, fold_output = False)\n",
    "\n",
    "print('Predicted differences: ', np.linalg.norm(vb_pert_pred - vb_opt))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vb_pert_pred_dict = vb_params_paragami.fold(vb_pert_pred, free = True)\n",
    "e_z_pert_pred = gmm_lib.get_optimal_z_from_vb_params_dict(features, vb_pert_pred_dict, gh_loc, gh_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get flattened perturbed KL\n",
    "get_perturbed_vb_loss = paragami.Functor(\n",
    "    original_fun=fun_sens_lib.get_perturbed_kl, argnums=[1])\n",
    "\n",
    "get_perturbed_vb_loss.cache_args(features, None, epsilon_param_dict, rescaled_log_phi, \n",
    "                     prior_params_dict, gh_loc, gh_weights)\n",
    "\n",
    "get_perturbed_vb_loss_flattened = \\\n",
    "    paragami.FlattenedFunction(original_fun=get_perturbed_vb_loss,\n",
    "                                patterns=vb_params_paragami,\n",
    "                                free=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "vb_pert_opt = optimization_lib.optimize_full(get_perturbed_vb_loss_flattened, deepcopy(vb_opt),\n",
    "                    bfgs_max_iter = 500, netwon_max_iter = 50,\n",
    "                    max_precondition_iter = 10,\n",
    "                    gtol=1e-8, ftol=1e-8, xtol=1e-8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "vb_pert_opt_dict = vb_params_paragami.fold(vb_pert_opt, free = True)\n",
    "e_z_pert_opt = gmm_lib.get_optimal_z_from_vb_params_dict(features, vb_pert_opt_dict, gh_loc, gh_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Epsilon:', epsilon)\n",
    "\n",
    "plt.figure()\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(vb_pert_opt - vb_opt, vb_pert_pred - vb_opt, 'k.')\n",
    "plt.plot(vb_pert_opt - vb_opt, vb_pert_opt - vb_opt, 'r')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('original expected number of clusters: ',\n",
    "    cluster_lib.get_e_number_clusters_from_logit_sticks(vb_opt_dict['stick_propn_mean'], \n",
    "                                                        vb_opt_dict['stick_propn_info'], \n",
    "                                                        n_obs = 150, \n",
    "                                                        n_samples = 10000))\n",
    "\n",
    "print('perturbed expected number of clusters: ',\n",
    "    cluster_lib.get_e_number_clusters_from_logit_sticks(vb_pert_opt_dict['stick_propn_mean'], \n",
    "                                                        vb_pert_opt_dict['stick_propn_info'], \n",
    "                                                        n_obs = 150, \n",
    "                                                        n_samples = 10000))\n",
    "\n",
    "print('predicted expected number of clusters: ',\n",
    "    cluster_lib.get_e_number_clusters_from_logit_sticks(vb_pert_pred_dict['stick_propn_mean'], \n",
    "                                                        vb_pert_pred_dict['stick_propn_info'], \n",
    "                                                        n_obs = 150, \n",
    "                                                        n_samples = 10000))\n",
    "\n",
    "\n",
    "      "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Predict for a range of perturbations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The range of epsilons we shall consider\n",
    "epsilon_vec = np.linspace(0, 1, 11) ** 2 # Square to get more points close to 0\n",
    "print(epsilon_vec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def refit_with_epsilon(epsilon_param_dict, init_vb_opt):\n",
    "    # sets new epsilon, returns new vb optimum\n",
    "        \n",
    "    # set up new objective, with the prior changed\n",
    "    get_perturbed_vb_loss = paragami.Functor(\n",
    "        original_fun=fun_sens_lib.get_perturbed_kl, argnums=[1])\n",
    "    \n",
    "    get_perturbed_vb_loss.cache_args(features, None, epsilon_param_dict, rescaled_log_phi, \n",
    "                         prior_params_dict, gh_loc, gh_weights)\n",
    "\n",
    "    get_perturbed_vb_loss_flattened = \\\n",
    "        paragami.FlattenedFunction(original_fun=get_perturbed_vb_loss,\n",
    "                                    patterns=vb_params_paragami,\n",
    "                                    free=True)\n",
    "    \n",
    "    # optimize\n",
    "    vb_opt_pert = optimization_lib.optimize_full(get_perturbed_vb_loss_flattened, deepcopy(init_vb_opt),\n",
    "                    bfgs_max_iter = 0, netwon_max_iter = 50,\n",
    "                    max_precondition_iter = 10,\n",
    "                    gtol=1e-8, ftol=1e-8, xtol=1e-8)\n",
    "    \n",
    "    return vb_opt_pert"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "vb_pert_opt_array = []\n",
    "vb_pert_pred_array = []\n",
    "\n",
    "\n",
    "for i in range(len(epsilon_vec)): \n",
    "    epsilon = epsilon_vec[i]\n",
    "    print('\\n\\nEpsilon: ', epsilon)\n",
    "    \n",
    "    epsilon_param_dict = {'epsilon': np.array([epsilon])}\n",
    "    \n",
    "    # get linear predictions\n",
    "    vb_pert_pred = \\\n",
    "        epsilon_sens.predict_opt_par_from_hyper_par(epsilon_param_dict, fold_output = False)\n",
    "\n",
    "    print('Predicted differences: ', np.linalg.norm(vb_pert_pred - vb_opt))\n",
    "    \n",
    "    # Then fit with the perturbation.\n",
    "    vb_pert_opt = refit_with_epsilon(epsilon_param_dict, vb_opt)\n",
    "    \n",
    "    print('Done. ')\n",
    "\n",
    "    print('Predicted differences: ', np.linalg.norm(vb_pert_pred - vb_opt))\n",
    "    print('Actual differences: (This is nonzero if phi did anything):',\n",
    "          np.linalg.norm(vb_pert_opt - vb_opt))\n",
    "    \n",
    "    # save results: \n",
    "    vb_pert_opt_array.append(vb_pert_opt)\n",
    "    vb_pert_pred_array.append(vb_pert_pred)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lets look at the expected number of clusters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Number of in sample clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "threshold = 0\n",
    "\n",
    "lr_e_num_clusters_vec = np.zeros(len(epsilon_vec))\n",
    "refit_e_num_clusters_vec = np.zeros(len(epsilon_vec))\n",
    "\n",
    "for i in tqdm(range(len(epsilon_vec))): \n",
    "    # number of clusters as predicted by linear response\n",
    "    lr_e_num_clusters, _ = gmm_lib.get_e_num_clusters_from_free_par(features, vb_params_paragami, \n",
    "                                    vb_pert_pred_array[i],\n",
    "                                    gh_loc, gh_weights,\n",
    "                                    threshold = threshold,\n",
    "                                    n_samples = 100000)\n",
    "    \n",
    "    lr_e_num_clusters_vec[i] = lr_e_num_clusters\n",
    "    \n",
    "    # number of clusters after refitting \n",
    "    refit_e_num_clusters, _ = gmm_lib.get_e_num_clusters_from_free_par(features, vb_params_paragami, \n",
    "                                    vb_pert_opt_array[i],\n",
    "                                    gh_loc, gh_weights,\n",
    "                                    threshold = threshold,\n",
    "                                    n_samples = 100000)\n",
    "    \n",
    "    refit_e_num_clusters_vec[i] = refit_e_num_clusters\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(epsilon_vec, lr_e_num_clusters_vec, '+--')\n",
    "plt.plot(epsilon_vec, refit_e_num_clusters_vec, '+-')\n",
    "\n",
    "plt.xlabel('alpha')\n",
    "plt.ylabel('num posterior clusters')\n",
    "plt.legend(('lr', 'refit'))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Number of predicted clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "threshold = 0\n",
    "\n",
    "lr_e_num_pred_clusters_vec = np.zeros(len(epsilon_vec))\n",
    "refit_e_num_pred_clusters_vec = np.zeros(len(epsilon_vec))\n",
    "\n",
    "for i in tqdm(range(len(epsilon_vec))): \n",
    "     # number of clusters as predicted by linear response\n",
    "    lr_e_num_pred_clusters = gmm_lib.get_e_num_pred_clusters_from_vb_free_params(vb_params_paragami, \n",
    "                                    vb_pert_pred_array[i],\n",
    "                                    n_obs = np.shape(features)[0], \n",
    "                                    threshold = threshold,\n",
    "                                    n_samples = 100000)\n",
    "    \n",
    "    lr_e_num_pred_clusters_vec[i] = lr_e_num_pred_clusters\n",
    "    \n",
    "    # number of clusters after refitting \n",
    "    refit_e_num_pred_clusters = gmm_lib.get_e_num_pred_clusters_from_vb_free_params(vb_params_paragami, \n",
    "                                    vb_pert_opt_array[i],\n",
    "                                    n_obs = np.shape(features)[0], \n",
    "                                    threshold = threshold,\n",
    "                                    n_samples = 100000)\n",
    "    \n",
    "    refit_e_num_pred_clusters_vec[i] = refit_e_num_pred_clusters\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(epsilon_vec, lr_e_num_pred_clusters_vec, '+--')\n",
    "plt.plot(epsilon_vec, refit_e_num_pred_clusters_vec, '+-')\n",
    "\n",
    "plt.xlabel('balpha')\n",
    "plt.ylabel('num posterior clusters')\n",
    "plt.legend(('lr', 'refit'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "bnpgmm_runjingdev",
   "language": "python",
   "name": "bnpgmm_runjingdev"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
