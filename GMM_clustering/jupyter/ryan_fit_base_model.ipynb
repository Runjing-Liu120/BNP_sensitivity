{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "git_repo = '../../..'\n",
    "\n",
    "import sys\n",
    "import os\n",
    "\n",
    "#sys.path.insert(0, './../../../LinearResponseVariationalBayes.py')\n",
    "sys.path.insert(0, os.path.join(git_repo, 'BNP_sensitivity/GMM_clustering/'))\n",
    "#'./../')\n",
    "\n",
    "# Linear response libraries\n",
    "import LinearResponseVariationalBayes as vb\n",
    "import LinearResponseVariationalBayes.SparseObjectives as obj_lib\n",
    "#import LinearResponseVariationalBayes.ExponentialFamilies as ef\n",
    "\n",
    "# My libraries\n",
    "import gmm_clustering_lib as gmm_utils\n",
    "import modeling_lib \n",
    "import functional_sensitivity_lib as fun_sens_lib \n",
    "import utils_lib\n",
    "\n",
    "from scipy import spatial\n",
    "import scipy.cluster.hierarchy as sch\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "import time\n",
    "from tqdm import tqdm\n",
    "\n",
    "from copy import deepcopy\n",
    "\n",
    "# Just while you're experimenting\n",
    "from autograd import numpy as np\n",
    "from autograd import scipy as sp\n",
    "\n",
    "# import numpy as np\n",
    "# import scipy as sp\n",
    "\n",
    "np.random.seed(453453)\n",
    "\n",
    "import json "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "iris_features, iris_species = utils_lib.load_data()\n",
    "dim = iris_features.shape[1]\n",
    "\n",
    "# Get some things that will be useful for plotting.\n",
    "pca_fit, pc_features, colors1, colors2 = utils_lib.get_plot_data(iris_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Set up the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "prior_params = gmm_utils.get_default_prior_params(dim)\n",
    "prior_params['alpha'].set(8.0)\n",
    "prior_params['prior_gamma_df'].set(8)\n",
    "prior_params['prior_gamma_inv_scale'].set(np.eye(dim) * 0.62)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "k_approx = 12\n",
    "gh_deg = 8\n",
    "model = gmm_utils.DPGaussianMixture(\n",
    "    iris_features, k_approx, prior_params, gh_deg, use_logitnormal_sticks=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# run k-means init\n",
    "n_kmeans_init = 50\n",
    "init_global_free_param = model.cluster_and_set_inits(n_kmeans_init = n_kmeans_init)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BGFS\n",
      "Iter: 0\t RMSE: 5.4171664084976205\t Objective: 4927.074571257251\n",
      "Iter: 10\t RMSE: 10.722601299848941\t Objective: 3164.887763988899\n",
      "Iter: 20\t RMSE: 12.727784407538786\t Objective: 2095.64267039293\n",
      "Iter: 30\t RMSE: 12.825320826126164\t Objective: 403.25309073085555\n",
      "Iter: 40\t RMSE: 12.336111507461514\t Objective: -156.2818830513076\n",
      "Iter: 50\t RMSE: 12.153510031465627\t Objective: -265.9051915783531\n",
      "Iter: 60\t RMSE: 11.665404774476754\t Objective: -278.95947380457665\n",
      "Iter: 70\t RMSE: 11.243659114866814\t Objective: -287.3217971112322\n",
      "Iter: 80\t RMSE: 11.270215074059495\t Objective: -293.1724859597803\n",
      "Iter: 90\t RMSE: 11.279599774958886\t Objective: -297.93642331028184\n",
      "Iter: 100\t RMSE: 11.270952653637949\t Objective: -301.5500897148094\n",
      "Warning: Maximum number of iterations has been exceeded.\n",
      "         Current function value: -302.542099\n",
      "         Iterations: 100\n",
      "         Function evaluations: 110\n",
      "         Gradient evaluations: 110\n",
      "Conditioned Newton:\n",
      "i =  0\n",
      "Iter: 110\t RMSE: 11.253624423017019\t Objective: -302.5420985282098\n",
      "Iter: 120\t RMSE: 11.246263059285315\t Objective: -305.2481721564943\n",
      "Iter: 130\t RMSE: 11.252832510017695\t Objective: -305.3431900441284\n",
      "Warning: A bad approximation caused failure to predict improvement.\n",
      "         Current function value: -305.357687\n",
      "         Iterations: 27\n",
      "         Function evaluations: 29\n",
      "         Gradient evaluations: 24\n",
      "         Hessian evaluations: 0\n",
      "Iter 1: x_diff = 31.82017370840149, f_diff = 2.8155880641871818\n",
      "i =  1\n",
      "Iter: 140\t RMSE: 11.255325883751928\t Objective: -305.3576865923987\n",
      "Warning: A bad approximation caused failure to predict improvement.\n",
      "         Current function value: -305.357687\n",
      "         Iterations: 0\n",
      "         Function evaluations: 2\n",
      "         Gradient evaluations: 1\n",
      "         Hessian evaluations: 0\n",
      "Iter 2: x_diff = 5.406742761837613e-14, f_diff = 9.663381206337363e-13\n",
      "Done. \n"
     ]
    }
   ],
   "source": [
    "t0 = time.time()\n",
    "best_param, kl_hessian, kl_hessian_corrected, \\\n",
    "init_opt_time, newton_time, x_conv, f_conv, vb_opt = \\\n",
    "model.optimize_full(init_global_free_param,\n",
    "    init_max_iter=100,\n",
    "    final_max_iter=500)\n",
    "t_newton = time.time() - t0\n",
    "\n",
    "print('Done. ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_param = deepcopy(model.global_vb_params.get_free())\n",
    "prior_free_params = deepcopy(model.prior_params.get_free())\n",
    "model.set_from_global_free_par(best_param)\n",
    "\n",
    "moment_model = gmm_utils.InterestingMoments(model)\n",
    "#linear_sens = gmm_utils.LinearSensitivity(model, moment_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "kl_hessian = model.objective.fun_free_hessian(best_param)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 7.60384671e+02, -5.25771180e-12, -2.12037829e-16,\n",
       "        -2.31582102e-12],\n",
       "       [-5.25771180e-12,  3.39103307e+00,  1.91647366e-03,\n",
       "         2.60831462e-02],\n",
       "       [-2.12037829e-16,  1.91647366e-03,  3.63505686e+00,\n",
       "         1.35393420e-02],\n",
       "       [-2.31582102e-12,  2.60831462e-02,  1.35393420e-02,\n",
       "         1.23617881e+00]])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "kl_hessian[1:5, 1:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(190,)\n"
     ]
    }
   ],
   "source": [
    "dgdeta = np.zeros(kl_hessian.shape[0])\n",
    "dgdeta[0] = 1\n",
    "\n",
    "gh = np.linalg.solve(kl_hessian, dgdeta)\n",
    "print(gh.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(11,)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.k_approx\n",
    "model.global_vb_params['v_sticks']['mean'].get().shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class StickSensitivity(object):\n",
    "#     def __init__(self, model, best_param, kl_hessian, dgdeta):\n",
    "#         self.model = model\n",
    "#         self.best_param = best_param\n",
    "#         self.log_q_logit_stick_obj = obj_lib.Objective(\n",
    "#             self.model.global_vb_params, self.get_log_q_logit_stick)\n",
    "#         self.set_lr_matrix(kl_hessian, dgdeta)\n",
    "\n",
    "#     def set_lr_matrix(self, kl_hessian, dgdeta):\n",
    "#         self.lr_mat = -1 * np.linalg.solve(kl_hessian, dgdeta)\n",
    "\n",
    "#     # The log variational density of stick k at logit_v\n",
    "#     # in the logit_stick space.\n",
    "#     def get_log_q_logit_stick(self, logit_v, k):\n",
    "#         mean = self.model.global_vb_params['v_sticks']['mean'].get()[k]\n",
    "#         info = self.model.global_vb_params['v_sticks']['info'].get()[k]\n",
    "#         return -0.5 * (info * (logit_v - mean) ** 2 - np.log(info))\n",
    "\n",
    "#     # Return a vectof of log variational densities for all sticks at logit_v\n",
    "#     # in the logit stick space.\n",
    "#     def get_log_q_logit_all_sticks(self, logit_v):\n",
    "#         mean = self.model.global_vb_params['v_sticks']['mean'].get()\n",
    "#         info = self.model.global_vb_params['v_sticks']['info'].get()\n",
    "#         return -0.5 * (info * (logit_v - mean) ** 2 - np.log(info))\n",
    "\n",
    "#     # The base prior (of any stick -- they are all the same) at logit_v in\n",
    "#     # the logit stick space.\n",
    "#     def get_log_p0_logit_stick(self, logit_v):\n",
    "#         alpha = self.model.prior_params['alpha'].get()\n",
    "#         return logit_v - (alpha + 1) * np.log1p(np.exp(logit_v))\n",
    "\n",
    "#     # Get the influence function for a perturbation to the prior on stick k.\n",
    "#     def get_single_stick_influence(self, logit_v, k):\n",
    "#         log_q = self.get_log_q_logit_stick(logit_v, k)\n",
    "#         log_p0 = self.get_log_p0_logit_stick(logit_v)\n",
    "\n",
    "#         log_q_grad = self.log_q_logit_stick_obj.fun_free_grad(\n",
    "#             self.best_param, logit_v=logit_v, k=k)\n",
    "#         return(self.lr_mat.T @ log_q_grad * np.exp(log_q - log_p0))\n",
    "\n",
    "#     # Get the influence function for a perturbation to all stick priors\n",
    "#     # simultaneously.\n",
    "#     def get_all_stick_influence(self, logit_v):\n",
    "#         log_q_vec = self.get_log_q_logit_all_sticks(logit_v)\n",
    "\n",
    "#         # The prior is the same for every stick.\n",
    "#         log_p0 = self.get_log_p0_logit_stick(logit_v)\n",
    "\n",
    "#         log_q_grad_vec = np.array([\n",
    "#             self.log_q_logit_stick_obj.fun_free_grad(\n",
    "#                 self.best_param, logit_v=logit_v, k=k)\n",
    "#             for k in range(self.model.k_approx - 1) ])\n",
    "\n",
    "#         dens_ratios = np.exp(log_q_vec - log_p0)\n",
    "#         return(self.lr_mat.T @ np.einsum('kd,k->d', log_q_grad_vec, dens_ratios))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "module 'modeling_lib' has no attribute 'StickSensitivity'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-14-1e207aa2505f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mq_class\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodeling_lib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mStickSensitivity\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbest_param\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkl_hessian\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdgdeta\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mq_class\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mglobal_vb_params\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_free\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbest_param\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mq_class\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_log_q_logit_stick\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0.5\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mq_class\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog_q_logit_stick_obj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfun_free\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbest_param\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogit_v\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.5\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: module 'modeling_lib' has no attribute 'StickSensitivity'"
     ]
    }
   ],
   "source": [
    "\n",
    "q_class = gmm_utils.StickSensitivity(model, best_param, kl_hessian, dgdeta)\n",
    "q_class.model.global_vb_params.set_free(best_param)\n",
    "print(q_class.get_log_q_logit_stick(0.5, 0))\n",
    "print(q_class.log_q_logit_stick_obj.fun_free(best_param, logit_v=0.5, k=0))\n",
    "\n",
    "#print(best_param)\n",
    "q_class.model.global_vb_params.set_free(best_param)\n",
    "#print(q_class.log_q_logit_stick_obj.fun_free_grad(best_param, logit_v=0.5, k=0))\n",
    "\n",
    "print('---------\\n')\n",
    "print(q_class.get_log_q_logit_stick(0.5, 0))\n",
    "print(q_class.get_log_p0_logit_stick(0.1))\n",
    "\n",
    "print('\\n--------- all:')\n",
    "print(q_class.get_log_q_logit_all_sticks(0.1))\n",
    "\n",
    "print('---------\\n')\n",
    "print(q_class.get_single_stick_influence(logit_v=0, k=0))\n",
    "print(q_class.get_all_stick_influence(logit_v=0.1))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "v_grid = np.linspace(1e-3, 1 - 1e-3, num=100)\n",
    "logit_v_grid = np.log(v_grid / (1 - v_grid))\n",
    "\n",
    "plt.figure()\n",
    "influence_grid = np.array(\n",
    "    [ q_class.get_all_stick_influence(logit_v=logit_v) for\n",
    "      logit_v in logit_v_grid ])\n",
    "plt.plot(v_grid, influence_grid, 'k')\n",
    "plt.figure()\n",
    "plt.plot(logit_v_grid, influence_grid, 'k')\n",
    "\n",
    "logit_v_min = np.log(1e-3 / (1 - 1e-3))\n",
    "logit_v_max = np.log((1 - 1e-3) / 1e-3)\n",
    "logit_v_grid = np.linspace(logit_v_min, logit_v_max, num=100)\n",
    "v_grid = np.exp(logit_v_grid) / (1 + np.exp(logit_v_grid))\n",
    "\n",
    "influence_grid = np.array(\n",
    "    [ q_class.get_all_stick_influence(logit_v=logit_v) for\n",
    "      logit_v in logit_v_grid ])\n",
    "plt.figure()\n",
    "plt.plot(v_grid, influence_grid, 'k')\n",
    "plt.figure()\n",
    "plt.plot(logit_v_grid, influence_grid, 'k')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "v_grid = np.linspace(1e-3, 1 - 1e-3, num=100)\n",
    "logit_v_grid = np.log(v_grid / (1 - v_grid))\n",
    "\n",
    "for k in range(model.k_approx - 1):\n",
    "    influence_grid = np.array(\n",
    "        [ q_class.get_single_stick_influence(logit_v=logit_v, k=k) for\n",
    "          logit_v in logit_v_grid ])\n",
    "    plt.figure()\n",
    "    plt.plot(logit_v_grid, influence_grid, 'k')\n",
    "    plt.title(k)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
